MIME Version Server CERN Date Sunday Dec GMT Content Type text html Content Length Last Modified Monday Sep GMT Ashish JhaveriAshish Jhaveri ashish cornell edu Resume Postscript Format AddressEducationCourseworkProjectsExperienceComputer SkillsAwardsActivities InterestsAddressDepartment Address Upson Hall Floor Department Computer Science Cornell University Ithaca Residence Address Maple Avenue Apt Ithaca Tel HomeEducationCornell University College Engineering Ithaca Master Engineering Computer Science May Maharashtra Institute Technology Pune India Bachelor Engineering Computer Engineering with distinction July CourseworkProgramming Languages Software Engineering Engineering Computer Networks Advanced Database Management Systems Multimedia Systems Advanced Computer Architecture Operating Systems Computer Organization Data Structures Artificial Intelligence Microprocessor Systems Unix Operating System Computer Graphics HomeProjectsInterface for Storage Device Modelling Tool Fall Created interface for the the mainframe Done Fujitsu ICIM using Visual Basic Assembler for Spring Assembler for implemented Payroll System Fall Implemented the Payroll System COBOL ExperienceAssistant Software Engineer July July CMC Ltd Bombay India Development and Customization Computerized Accounting Loan Instruments with Backdated Reworking for Life Insurance Corporation India Developed using Uniface and IngresHomeComputer SkillsLanguages COBOL Visual Basic AssemblyPlatforms MSDOS Windows RDBMS Foxpro IngresOther Uniface Ingres SQL ESQLC HTMLAwardsAward for successful completion project CALIBRE CMC Ltd India Activities InterestsPresented seminar Digital Video Interactive Technology Maharashtra Institute Technology Pune India Trekking Cricket Music Home MIME Version Server CERN Date Tuesday Jan GMT Content Type text html Content Length Last Modified Wednesday Aug GMT Uncertain ReasoningUncertain Reasoning view paper click the open book image Combining Symbolic and Connectionist Learning Methods Refine Certainty Factor Rule Bases Jeffrey Mahoney Thesis Department Computer Sciences University Texas Austin May This research describes the system RAPTURE which designed revise rule bases expressed certainty factor format Recent studies have shown that learning facilitated when biased with domain specific expertise and have also shown that many real world domains require some form probabilistic uncertain reasoning order successfully represent target concepts RAPTURE was designed take advantage both these results Beginning with set certainty factor rules along with accurately labelled training examples RAPTURE makes use both symbolic and connectionist learning techniques for revising the rules order that they correctly classify all the training examples modified version backpropagation used adjust the certainty factors the rules information gain heuristic used add new rules and the Upstart algorithm used create new hidden terms the rule base Results refining four real world rule bases are presented that demonstrate the effectiveness this combined approach Two these rule bases were designed identify particular areas strands DNA one for identifying infectious diseases and the fourth attempts diagnose soybean diseases The results RAPTURE are compared with those backpropagation KBANN and other learning systems RAPTURE generally produces sets rules that are more accurate that these other systems often creating smaller sets rules and using less training time Refinement Bayesian Networks Combining Connectionist and Symbolic Techniques Sowmya Ramanchandran proposal Department Computer Sciences University Texas Austin Bayesian networks provide mathematically sound formalism for representing and reasoning with uncertain knowledge and are such widely used However acquiring and capturing knowledge this framework difficult There growing interest formulating techniques for learning Bayesian networks inductively While the problem learning Bayesian network given complete data has been explored some depth the problem learning networks with unobserved causes still open this proposal view this problem from the perspective theory revision and present novel approach which adapts techniques developed for revising theories symbolic and connectionist representations Thus assume that the learner given initial approximate network usually obtained from expert Our technique inductively revises the network fit the data better Our proposed system has two components one component revises the parameters Bayesian network known structure and the other component revises the structure the network The component for parameter revision maps the given Bayesian network into multi layer feedforward neural network with the parameters mapped weights the neural network and uses standard backpropagation techniques learn the weights The structure revision component uses qualitative analysis suggest revisions the network when fails predict the data accurately The first component has been implemented and will present results from experiments real world classification problems which show our technique effective will also discuss our proposed structure revision algorithm our plans for experiments evaluate the system well some extensions the system Revising Bayesian Network Parameters Using Backpropagation Sowmya Ramachandran and Raymond MooneyProceedings the International Conference Neural Networks ICNN Special Session Knowledge Based Artificial Neural Networks Washington June The problem learning Bayesian networks with hidden variables known hard problem Even the simpler task learning just the conditional probabilities Bayesian network with hidden variables hard this paper present approach that learns the conditional probabilities Bayesian network with hidden variables transforming into multi layer feedforward neural network ANN The conditional probabilities are mapped onto weights the ANN which are then learned using standard backpropagation techniques avoid the problem exponentially large ANNs focus Bayesian networks with noisy and noisy and nodes Experiments real world classification problems demonstrate the effectiveness our technique Comparing Methods For Refining Certainty Factor Rule Bases Jeffrey Mahoney and Raymond Mooney Proceedings the Eleventh International Workshop Machine Learning Rutgers July This paper compares two methods for refining uncertain knowledge bases using propositional certainty factor rules The first method implemented the RAPTURE system employs neural network training refine the certainties existing rules but uses symbolic technique add new rules The second method based the one used the KBANN system initially adds complete set potential new rules with very low certainty and allows neural network training filter and adjust these rules Experimental results indicate that the former method results significantly faster training and produces much simpler refined rule bases with slightly greater accuracy Modifying Network Architectures For Certainty Factor Rule Base Revision Jeffrey Mahoney and Raymond Mooney Proceedings the International Symposium Integrating Knowledge and Neural Heuristics Pensacola May ISIKNH This paper describes RAPTURE system for revising probabilistic rule bases that converts symbolic rules into connectionist network which then trained via connectionist techniques uses modified version backpropagation refine the certainty factors the rule base and uses information gain heuristic Quinlan add new rules Work currently under way for finding improved techniques for modifying network architectures that include adding hidden units using the UPSTART algorithm Frean case made via comparison with fully connected connectionist techniques for keeping the rule base close the original possible adding new input units only needed Combining Connectionist and Symbolic Learning Refine Certainty Factor Rule Bases Jeffrey Mahoney and Raymond Mooney Connection Science Special issue Architectures for Integrating Neural and Symbolic Processing This paper describes Rapture system for revising probabilistic knowledge bases that combines connectionist and symbolic learning methods Rapture uses modified version backpropagation refine the certainty factors Mycin style rule base and uses information gain heuristic add new rules Results refining three actual expert knowledge bases demonstrate that this combined approach generally performs better than previous methods Combining Symbolic and Neural Learning Revise Probabilistic Theories Jeffrey Mahoney Raymond Mooney Proceedings the Machine Learning Workshop Integrated Learning Real Domains Aberdeen Scotland July This paper describes RAPTURE system for revising probabilistic theories that combines symbolic and neural network learning methods RAPTURE uses modified version backpropagation refine the certainty factors Mycin style rule base and uses information gain heuristic add new rules Results two real world domains demonstrate that this combined approach performs well better than previous methods estlin utexas edu Date Tue Jan GMT Server Apache dev Connection close Content Type text html Last Modified Tue Sep GMT ETag ebf Content Length Accept Ranges bytes Lynn Andrea Stein studentsStudents past and present Eytan Adar works Haystack Ben Adida works the Rethinking project and fall Jen Alexander spent the summer MIT through the CRA NSF Distributed Mentor Project She built system Sound Localization Meeting Room during the summer Mark Asdoorian works Haystack Joshua Reuben Brown helped develop and for the Rethinking project the summer Dwaine Clarke works Haystack Michael Coen works Sodabot software agent environment and construction system Click here for extended abstract Sal Desiano resurrected and wrote new parser for Natural Communication with Robots part the Cognitive Robotics project the summer Matthew Domsch the first AAAI Robot Building Lab the National Conference Artificial Intelligence Washington also wrote the Organizer Manual For more information the student run MIT robotics course taught each January see the home page Matt Eldredge supported some our robots Ian Horswill built Polly robot that gives tours the Lab His thesis Specialization Perceptual Processes was nominated for the ACM Distinguished Dissertation Award and won the MIT Department Electrical Engineering and Computer Science Sprowls Award Mike Evans worked the spring Heji Kim spent the summer MIT through the CRA NSF Distributed Mentor Project She worked Motion Based Multi person Tracking Joshua Kramer works SodaBot Carol Lee the first AAAI Robot Building Lab the National Conference Artificial Intelligence Washington Lili Liu works Haystack Antonio Mercado Brown University implemented Hybrid object oriented language that merges delegation and inheritance Salil Pitroda did his Advanced Undergraduate Project AUP RobotWorld for the Rethinking project the spring Eric Prebys works Haystack and the line bibliography project Lydia Sandon works the Rethinking project and fall Brian Scassellati Eng deduced High level Perceptual Contours from Variety Low level Physical Features Morris Joseph Levin Memorial Award for Best Master Works Oral Thesis Presentation intends his work part the visual system for Cog Emil Sit works the Rethinking project including summer and fall Ellen Spertus works information access and the world wide web Maciej Stachowiak works the Rethinking project including summer and fall Nora Szasz works the Rethinking project and fall Tim Tang supported the SensorBots Mark Torrance taught communicate more naturally Natural Communication with Robots did tour duty with Cog before settling work the Intelligent Room and Active Notebook Karsten Ulland the first AAAI Robot Building Lab the National Conference Artificial Intelligence Washington and the MIT Robot Building Collaborative the Fall Chuck Van Buren works Haystack Mike Wessler built Modular Visual Tracking System Click here for less formal description now exploring motor control Holly Yanco worked multiagent learning and adaptable synthetic robot languages See also the membership list the group urop page Back Lynn Andrea Stein Home Pagelas mit edu Date Mon Nov GMT Server NCSA Content type text html Last modified Thu Apr GMT Content length Section Program Program Wind Chill FactorDue Date Monday amGrade your final grade Text Covered Chp and Problem Description you have already probably noticed winters Wisconsin can get very cold especially when windy outside For this assignment you are write program compute the wind chill factor WCF from the current temperature and wind speed The WCF indicates how cold will feel you are outside windy day Your program will ask the user enter the current temperature degrees Fahrenheit followed the wind speed mph Both values should read INTEGERs Your program will then compute and print the WCF degrees Fahrenheit given the formula where Vel the wind speed and Temp the temperature After you print the WCF degrees Fahrenheit convert this temperature degrees Celsius and print using the formula The values you compute for the WCF deg and deg will REAL numbers however print these values INTEGER rounded the nearest whole degree Finally print English description the current weather particular the wind speed less than mph then calm less than mph then windy greater than equal mph then very windy Similarly the WCF less than deg then bitterly cold less than deg then cold greater than equal deg then cool This description must printed single line Example the current temperature deg and the wind speed mph then your program should display hint use this check your results The wind chill factor deg The wind chill factor deg The weather windy and bitterly cold worry you get extra spaces your output shown above Later will see how get rid these spaces using formatted output What Hand Hand your assignment online copying your FORTRAN source code file program for and the compiled executable file program exe your handin directory Your files are timestamped with the current time when you copy them your handin directory This will used check they were handed time whether late day penalties apply Important Make sure you copy the correct versions your program files because these are what will graded You also have hand printed copy your FORTRAN source code file and printed copy the output when your program run with the five sets test data given the gradesheet Gradesheet The following gradesheet will used grade this assignment Please take close look make sure you everything that required Note that you will graded correct output for the input values given below before handing you must run your program with these values and manually check that your program gives the correct results every case use calculator Correct Output temperature deg wind speed mph temperature deg wind speed mph temperature deg wind speed mph temperature deg wind speed mph temperature deg wind speed mph User Interface Useful prompts for the temperature and wind speed Meaningful display the WCF and weather description Implementation Correct implementation formulas Correct use THEN ELSE END statement Correct use other FORTRAN statements Meaningful variable names Indenting and neatness Documentation Program description Variable definitions and section comments Total Copyright copy Gareth Bestor bestor wisc edu Last modified April 