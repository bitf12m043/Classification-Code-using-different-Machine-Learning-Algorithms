MIME Version Server CERN Date Sunday Dec GMT Content Type text html Content Length Last Modified Wednesday Oct GMT SculptureSculptureJason Hickey Many thanks Gail Scott White MIME Version Server CERN Date Tuesday Jan GMT Content Type text html Content Length Last Modified Monday Mar GMT Notes UNITY The following list the notes UNITY that are available electronically all are compressed conforming Postscript These files are for non commercial redistribution only They may not altered any form without permission from the authors Mail comments questions psp request utexas edu General Conjunction and Disjunction Rules for unless Misra Theorem About Dynamic Acyclic Graphs Misra Composition Theorem About Fixed Points Misra Theorem Relating leads and unless Misra Progress Safety Safety Misra Leads and program union Singh Strengthening the Guard Singh Functions Preserved unless leads Misra Proving unless Properties Parts Misra Monotonicity Stability and Constants Misra The Importance Ensuring Misra Phase Synchronization Misra Family process Mutual Exclusion Algorithms Misra Soundness the Substitution Axiom Misra Auxiliary Variables Misra Proving Progress For Program Sequencing Misra Preserving Progress Under Program Composition Misra Specialization detects Misra More Strengthening the Guard Misra Examples Program Construction Using UNITY Knapp Stable Conjunction Misra Correction Note Dappert Farquhar Three Definitions leads for UNITY Pachl This note currently unavailable Notion Completeness for the Leads Rao This note currently unavailable Completion Theorem Revisited Misra Using Prefix Computation Add Misra Methodological Hints About Constructing unless Properties Misra Program Composition Theorem Involving Fixed Point Misra This Note Subsumes Note Generalization the Completion Theorem Misra More detects and trails Misra How reason with Strong fairness and fairness Misra Proof Real Time Mutual Exclusion Algorithm John Allen Carruth Misra Proving Convergence Hopfield Neural Network Muller Examples Program Construction using UNITY Knapp unnumbered Date Tue Jan GMT Server Apache Content type text html Content length Last modified Wed May GMT Local Resources Local Resources Major Reconstruction Progress Back RLLC Homepage Comments criticism enquiries Mail marc brandeis edu Date Tue Nov GMT Server NCSA Content type text html Last modified Wed Aug GMT Content length Lecture notes Chapter Performance features Chapter performance features Architectural Features used enhance performance loosely following chapter What better computer What the best computer The factors involved are generally cost and performance COST FACTORS cost hardware design cost software design applications cost manufacture cost end purchaser PERFORMANCE FACTORS what programs will run how frequently will they run how big are the programs how many users how sophisticated are the users what devices are necessary this chapter discusses ways increasing performance There are two ways make computers faster Wait year Implement faster better newer technology More transistors will fit single chip More pins can placed around the The process used will have electronic devices transistors that switch faster new innovative architectures and architectural features MEMORY HIERARCHIES Known current technologies the time access data from memory order magnitude greater than CPU operation For example bit complement addition takes time unit then load bit word takes about time units Since every instruction takes least one memory access for the instruction fetch the performance computer dominated its memory access time try help this difficulty have load store architectures where most instructions take operands only from memory also try have fixed size SMALL size instructions what really want very fast memory the same speed the CPU very large capacity Mbytes low cost these are mutually incompatible The faster the memory the more expensive becomes The larger the amount memory the slower becomes What can compromise Take advantage the fact fact looking many real programs that memory accesses are not random They tend exhibit LOCALITY LOCALITY nearby kinds Locality time temporal locality data has been referenced recently likely referenced again soon example the instructions with loop The loop likely executed more than once Therefore each instruction gets referenced repeatedly short period time example The top stack repeatedly referenced within program Locality space spacial locality data has been referenced recently then data nearby memory likely referenced soon example array access The elements array are neighbors memory and are likely referenced one after the other example instruction streams Instructions are located memory next each other Our model for program execution says that unless the explicitly changed like branch jump instruction sequential instructions are fetched and executed can use these tendencies advantage keeping likely referenced soon data faster memory than main memory This faster memory called CACHE CPU cache memory located very close the CPU contains COPIES PARTS memory standard way accessing memory for system with cache The programmer doesn see know about any this instruction fetch load store goes the cache the data the cache then have HIT The data handed over the CPU and the memory access completed the data not the cache then have MISS The instruction fetch load store then sent main memory average the time memory access cache access time misses memory access time This average mean access time will change for each program depends the program and its reference pattern and how that pattern interracts with the cache parameters cache managed hardware Keep recently accessed block exploits temporal locality Break memory into aligned blocks lines bytes exploits spatial locality transfer data from cache blocks put block block frame state valid address tag data simple CACHE DIAGRAM here Example Memory words xffffffff xabababab byte cache block frame state tag data bytes words invalid tag cache mod xfffffff load block byte cache block frame state tag data bytes words valid xffffffff Return CPU put tag cache Yes return CPU Beyond the scope this class block and block frames divided sets equivalence classes speed lookup terms fully associative set associative direct mapped Often cache instruction cache cycle data cache cycle main memory cycles Performance for data references miss ratio misses mean access time cache access miss ratio memory access Typical cache size byte given Mbyte memory times faster the capacity often contains the references Remember recently accessed blocks are the cache temporal locality the cache smaller than main memory not all blocks are the cache blocks are larger than word spacial locality This idea exploiting locality can done many levels Implement hierarchical memory system smallest fastest most expensive memory registers relatively small fast expensive memory CACHE large fast possible cheaper memory main memory largest slowest cheapest per bit memory disk registers are managed assigned compiler asm lang programmer cache managed assigned hardware partially main memory managed assigned disk managed Programmer model one instruction fetched and executed time Computer architect model The effect program execution are given the programmer model But implementation may different make execution programs faster attempt exploit PARALLELISM doing more than one thing one time program level parallelism Have one program run parts itself more than one computer The different parts occasionally synch needed but they run the same time instruction level parallelism ILP Have more than one instruction within single program executing the same time PIPELINING ILP concept task broken down into steps Assume that there are steps each takes the same amount time Mark Hill EXAMPLE car wash steps prep wash rinse dry wax assume each step takes time unit time wash car red time units time wash cars red green blue time units which car time units red green blue PIPELINE overlaps the steps which car time units red green blue yellow etc STILL TAKES TIME UNITS WASH CAR BUT THE RATE CAR WASHES GOES Pipelining can done computer hardware stage pipeline steps instruction fetch instruction execute everything else which instruction time units time for instruction time units INSTRUCTION LATENCY rate instruction execution pipeline depth time for INSTRUCTION THROUGHPUT instruction per time unit stage pipeline currently popular pipelined implementation has stages has stages but different has stages steps instruction fetch instruction decode and get operands from registers ALU operation can effective address calculation memory access write back results written register which time units instruction INSTRUCTION LATENCY time units INSTRUCTION THROUGHPUT instruction per time unit unfortunately pipelining introduces other difficulties data dependencies suppose have the following code data addi the data loaded doesn get written until but the addi instruction wants get the data out its stage which time units instruction addi the simplest solution STALL the pipeline Also called HOLES HICCOUGHS BUBBLES the pipe which time units instruction addi pipeline stalling DATA DEPENDENCY also called HAZARD causes performance decrease more data dependencies Read After Write RAW example given read data needed before has been written Given for completeness not difficulty current pipelines practice since the only writing occurs the last stage Write After Read WAR Write After Write WAR NOTE there difficulty implementing stage pipeline due DATA dependencies control dependencies what happens pipeline the case branch instructions MAL CODE SEQUENCE label addi label mult which time units instruction changed here addi WRONG instruction fetched here whenever the changes except for BRANCHES and PIPELINING how minimize the effect control dependencies pipelines easiest solution poor performance Cancel anything later the pipe when branch jump decoded This works long nothing changes the program state before the cancellation Then let the branch instruction finish flush the pipe and start again which time units instruction changed here addi cancelled branch Prediction static dynamic add lots extra hardware try help static assume that the branch will not taken When the decision made the knows the correct instruction has been partially executed the correct instruction currently the pipe let and all those after continue Then there will holes the pipe the incorrect instruction currently the pipe meaning that the branch was taken then all instructions currently the pipe subsequent the branch must BACKED OUT dynamic variation Have some extra that keeps track which branches have been taken the recent past Design the presume that branch will taken the same way was previously the guess wrong back out Question for the advanced student Which better Why NOTE solution works quite well with currently popular pipeline solutions because state information changed until the very last stage instruction long the last stage hasn started backing out matter stopping the last stage from occuring and getting the right separate test from branch make the conditional test and address calculation separate instructions from the one that changes the This reduces the number holes the pipe delayed branch MIPS solution The concept prediction always wrong sometime There will holes the pipe when the prediction wrong the goal reduce eliminate the number holes the case branch The mechanism Have the effect branch the change the delayed until subsequent instruction This means that the instruction following branch executed independent whether the branch taken not NOTE the simulator completely ignores this delayed branch mechanism code example add beq label move label sub turned into the following MIPS assembler add beq label nop really pipeline hole the DELAY SLOT move label sub the assembler has any smarts all would REARRANGE the code beq label add move label sub This code can rearranged only there are data dependencies between the branch and the add instructions fact any instruction from before the branch and after any previous branch can moved into the DELAY SLOT long there are dependencies Delayed branching depends smart assembler make the hardware perform peak efficiency This general trend the field computer science Let the more and more improve performance the squashing fancy name for branch prediction that always presumes the branch will taken and keeps copy the that will needed the case backing out condition codes historically significant way branching Condition codes were used MANY machines before pipelining became popular bit registers condition code register negative overflow positive zero The result instruction set these bits Conditional branches were then based these flags Example label branch label the bit set Earlier computers had virtually every instruction set the condition codes This had the effect that the test for the branch needed come directly before the branch Example sub blt label label performance improvement sometimes this allowed the programmer explicitly specify which instructions should set the condition codes this way pipelined machine the test could separated from the branch resulting fewer pipeline holes due data dependencies Amdahl Law why the common case matters most speedup new rate old rate old execution time new execution time program some enhancement part our program The fraction time spent that part the code The speedup that part the code Let enhancement speedup fraction the time speedup speedup old time old time old time Examples speedup inf lim inf speedup This says that should concentrate the common case 