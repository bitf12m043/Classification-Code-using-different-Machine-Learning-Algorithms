MIME Version Server CERN Date Monday Nov GMT Content Type text html Content Length Last Modified Sunday Apr GMT Low Latency Communication over ATM Networks using Active MessagesLow Latency Communication over ATM Networks using Active Messages Thorsten von Eicken Veena Avula Anindya Basu and Vineet Buch Department Computer Science Cornell University Ithaca Abstract Recent developments communication architectures for parallel machines have made significant progress and reduced the communication overheads and latencies over order magnitude compared earlier proposals This paper examines whether these techniques can carry over clusters workstations connected ATM network even though clusters use standard operating system software are equipped with network interfaces optimized for stream communication not allow direct protected user level access the network and use networks without reliable transmission flow control first part this paper describes the differences communication characteristics between clusters workstations built from standard hardware and software components and state the art multiprocessors The lack flow control and operating system coordination affects the communication layer design significantly and requires larger buffers each end than multiprocessors second part evaluates prototype implementation the low latency Active Messages communication model Sun workstation cluster interconnected ATM network Measurements show application application latencies about microseconds for small messages which roughly comparable the Active Messages implementation the Thinking Machines multiprocessor Table Contents Introduction Technical Issues SSAM SPARCstation Active Messages Prototype Comparison other approaches Conclusions Bibliography Introduction The shift from slow broadcast based local area networks high bandwidth switched network architectures making the use clusters workstations platforms for parallel processing more and more attractive While number software packages already support parallel processing today workstations and networks the communication performance over two orders magnitude inferior state the art multiprocessors result only embarassingly parallel applications parallel applications that essentially never communicate can make use such environments Networking technologies such ATM offer the opportunity close the gap for example ATM cells are roughly the same size messages multiprocessors takes only few microseconds send receive cell ATM switches can configured From purely technical point view the gap between clusters workstations and multiprocessors certainly closing and the distinction between the two types systems becoming blurred Differences remain particular the design and construction multiprocessors allows better integration all the components because they can designed fit together addition the sharing physical components such power supplies cooling and cabinets has the potential reduce cost and allow denser packaging While the debate over the significance these technological differences still open becoming clear that the two approaches will yield qualitatively similar hardware systems Indeed possible take cluster workstations and load system software making look almost identical multiprocessor This means that continuous spectrum platforms spanning the entire range from workstations Ethernet state the art multiprocessors can become available and tha From pragmatic point view however significant differences are likely remain The most important attraction using cluster workstations instead multiprocessor lies the off the shelf availability all its major hardware and software components This means that all the components are readily available they are familiar and their cost lower because economies scale leveraged across the entire workstation user community Thus even from technical point view there continuous spectrum between clusters and multiprocessors the use off the shelf components clusters will maintain differences fact the use standard components clusters raises the question whether these can reasonably used for parallel processing Recent advances multiprocessor communication performance are principally due tighter integration programming models compilers operating system functions and hardware primitives not clear whether these advances can carried over clusters whether the use standard components squarely odds with achieving the level integration required enable modern parallel programming models Specifically new communication architectures such distributed shared memory explicit remote memory access and Active Messages reduced the costs from hundreds thousands microseconds just few dozen precisely through the integration all system components These new communication architectures are designed such that network interfaces can implement common primitives directly hardware they allow the operating system moved out the critica This paper examines whether the techniques developed improve communication performance multiprocessors particular Active Messages can carried over clusters workstations with standard networks and mostly standard system software This paper assumes the current state the art technology which clusters using ATM networks differ from multiprocessors three major aspects clusters use standard operating system software which implies less coordination among individual nodes particular with respect process scheduling and address translation ATM networks not provide the reliable delivery and flow control that are taken for granted multiprocessor networks andnetwork interfaces for workstations optimize stream communication TCP and are less well integrated into the overall architecture connect the bus instead the memory bus comparing communication clusters and multiprocessors this paper makes two major contributions first analyzes Section the implications that the differences between clusters and multiprocessors have the design communication layers similar those used multiprocessors andsecond describes Section the design Active Messages prototype implementation collection Sun workstations interconnected ATM network which yields application application latencies the order The use Active Messages workstation clusters briefly contrasted other approaches Section and Section concludes the paper Technical Issues Collections workstations have been used many different forms run large applications order establish basis for comparison multiprocessors this paper limits itself consider only collections workstations called clusters which consist homogeneous set machines dedicated run parallel applications located close proximity such the same machine room and interconnected ATM network Such cluster can employed large variety settings The cluster could simply provide high performance compute service for user community run large parallel applications more typical setting would computational resource distributed application One such example the Stormcast weather monitoring system Norway runs very large collection machines spread across large portion the country but uses cluster few dozen workstations machine room without high speed network this case run compute intensive weather prediction models and emit storm warnings The availability low latency communication among these workstations would enable the use parallel programming languages and more powerful parallel algorithms both which require closer coupling among processors than possible today Concentrating the compute cluster offers the largest potential for improvement because the latency over the long haul links dominated speed light and network congestion issues and because the wide area communication comparatively better served today distributed computing software Note that this paper does not argue that running concurrent applications heterogeneous environment across large distances and workstations that happen sitting idle not interesting design point fact has been used successfully but that the set communication issues occurring such context cannot compared those multiprocessor Given that the applications for clusters considered here exhibit characteristics similar those multiprocessors the programming models used would comparable not identical those popular for parallel computing This includes various forms message passing send receive PVM shared memory cache coherent shared memory remote reads and writes explicit global memory and parallel object oriented languages numerous extensions parallel machines several proposed communication architectures have achieved the low overheads low latencies and high bandwidths that are required for high performance implementations the above programming models particular cache coherent shared memory remote reads and writes and Active Messages offer round trip communication within few hundred instruction times that frequent communication fine granularity such object object cache line basis remains compatible with high performance these settings the overhead communication that the time spent the processor initiating communication essentially the cost pushing message data into the network interface the sending end and pulling out the receiving end Virtually cycles are spent any protocol handling all reliability and flow control are handled hardware The operating system need not involved every communication operation because the network interface hardware can enforce protection boundaries across the network The above communication architectures cannot moved straightforward manner from multiprocessors clusters workstations with ATM networks because three major differences between the two ATM networks offer neither reliable delivery nor flow control ATM network interfaces provide support for protected user level access the network and the workstation operating systems not coordinate process scheduling address translation globally Coping with these differences poses major technical challenges and may eventually require the integration some multiprocessor specific features into the clusters The following three subsections present the nature these differences more detail and discuss the resulting issues Reliability and flow control the network multiprocessor networks flow control implemented hardware link link basis Whenever the input buffer router fills the output the stream router disabled prevent buffer overflow The flow control thus has the effect blocking messages the network and eventually the back pressure propagates the sending nodes are prevented from injecting further messages This mechanism guarantees that messages are never dropped due buffer space limitations within the network the receiving end addition the electrical characteristics the network are designed ensure very low error rates such that the use simple error detection and correction mechanism implemented hardware can offer the same reliability within the network typical the processing nodes themselves contrast ATM network does not provide any form flow control and does not offer reliable delivery Instead higher protocol layers must detect cell loss corruption and cause their retransmission While this partitioning responsibilities may acceptable the case stream based communication TCP video audio questionable parallel computing setting The flow control and the error detection and correction multiprocessor networks serve cover four causes message loss buffer overflow the receiving software buffer overflow the receiving network interface buffer overflow within the network and message corruption due hardware errors ATM network simple window based end end flow control schemes and per message CRC used AAL can cover the first and last cases cell loss addition preventing buffer overflow the receiving network interface can achieved ensuring that the rate which cells can moved from the interface into main memory least large the maximal cell arrival rate Preventing buffer overflow within the network however not realistically possible using end end flow control This particularly problem parallel computing setting which all nodes tend communicate with all other nodes both highly regular and irregular patterns unpredictable intervals The degree contention within the network therefore cannot measured predicted with any accuracy either the sender the receiver and communication patterns which result high contention will result high cell loss rates causing extensive retransmissions Traditional flow control schemes used stream based communication avoid fruitless retransmission storms dynamically reducing the transmission rate connections which experience high cell loss rates This works these settings because following the law large numbers contention wide area network does not tend vary instantaneously and therefore the degree contention observed the recent past good predictor for contention the near future illustration the difficulties parallel computing setting consider the implementation parallel sort The most efficient parallel sort algorithms are based alternation local sorts the nodes and permutation phases which all nodes exchange data with all other nodes These permutation phases serve move the elements sorted towards their correct position The communication patterns observed are highly dynamic and their characteristics depend large degree the input data any point the attempted data rate into given node exceeds the link rate then the output buffers stream switches will start filling Because the communication patterns change very rapidly essentially with every cell futile attempt predict contention and given the all all communication pattern the probability internal contention among seemingly unrelated connections high Beyond the problems caused contention and the resulting retransmissions the lack reliable delivery guarantee ATM networks imposes certain overhead the communication primitives Specifically the sender must keep copy each cell sent until corresponding acknowledgment received case the cell must retransmitted This means that messages cannot transferred directly between processor registers and the network interface possible the rather memory copy must made well User level access the network interface Recently multiprocessor communication architectures have achieved significant reduction the communication overhead eliminating the operating system from the critical path order not compromise security the network interface must offer some form protection mechanism shared memory models the memory management unit extended map remote memory into the local virtual user address space such that the operating system can enforce security managing the address translation tables Message based network interfaces contain node address translation table which maps the user virtual node numbers onto the physical node address space Again the operating system enforces security controlling the address translation thereby preventing process from sending message arbitrary node The current generation message based network interfaces only control the destination node address and therefore require that all processes parallel program run the same time The next generation adds the sending process each message allowing the receiving network interface discriminate between messages destined for the currently running process that can retrieve these message directly and messages for dormant processes which must queued typically the operating system for later retrieval contrast the network interfaces available for workstations not yet incorporate any form protection mechanism Instead the operating system must involved the sending and reception every message The connection based nature ATM networks would principally allow the design protection mechanism limit the virtual circuits user process has access the operating system would still control virtual circuit set But because the architecture the networking layers current operating systems does not seem set allow user level network interface access appears unlikely that network interfaces with these features will become commonplace soon The challenge any high performance communication layer for clusters thus minimize the path through the kernel judiciously coordinating the user kernel interactions Coordination system software across all communicating nodes almost all communication architectures the message reception logic the critical performance bottleneck order able handle incoming messages full network bandwidth the processing required for each arriving message must minimized carefully The trick used multiprocessor systems ensure rapid message handling constrain the sender only send messages which are easy handle shared memory systems this done coordinating the address translation tables among all processing nodes such that the originating node can translate the virtual memory address remote access and directly place the corresponding physical memory address into the message The set communication primitives small and fixed read and write and forcing the sender perform the complicated part remote memory access namely the protection checks and the address translation the handling request relatively simple implement the virtual address were sent the receiving node could discover that the requested virtual memory location had been paged out disk with the result that the handling the message would become rather involved Active Messages multiprocessors the scheduling processes assumed coordinated among all nodes such that communicating processes execute simultaneously their respective nodes This guarantees that messages can handled immediately arrival the destination process itself order accomplish this the sender Active Message specifies user level handler the destination whose role extract the message from the network and integrate into the ongoing computation The handler can also implement simple remote service and send reply Active Message back However order prevent deadlock the communication patterns are limited requests and replies handler reply message not allowed send any further messages implementation Active Messages typically reserves the first word each message for the handler address and the handler the receiving end dispatched immediately message arrival dispose the message The fact that the message layer can call upon the handlers deal with messages FIFO order simplifies the buffering considerably over that required more traditional message passing models such PVM MPI These models allow processes consume messages arbitrary order and arbitrary times forcing the communication architecture implement very general buffer and message matching mechanisms high cost clusters the fact that the operating systems the individual nodes are not nearly coordinated contradicts the assumption that messages can always consumed quickly upon arrival the case Active Messages the destination process might have been suspended and cannot run the handler and shared memory model the memory location requested might not mapped Although exact coordination not possible without major changes the operating system core implementation either communication model likely able perform some coordination among nodes its own and influence the local operating system accordingly This may allow the communication layer assume that the common case everything works out fine but must able handle the difficult cases well Summary Even though superficially cluster workstations appears the technically comparable multiprocessor the reality that key characteristics are different and cause significant implementation difficulties the very comparable raw hardware link bandwidths bisection bandwidths and routing latencies conceal the lack clusters flow control reliability user level network access and operating system coordination These shortcomings will inevitably result lower communication performance their quantitative effect performance evaluated the next section which presents prototype implementation Active Messages cluster Sun workstations However the lack flow control ATM networks poses fundamental problem can catastrophic performance degradation occur due significant cell loss particular communication patterns SSAM SPARCstation Active Messages Prototype The SSAM prototype implements the critical parts Active Messages communication architecture cluster SPARCstations connected ATM network The primary goal evaluate whether possible provide parallel programming environment the cluster that comparable those found multiprocessors The prototype primarily concerned with providing performance par with parallel machines while addressing the handicaps ATM networks that have been identified the previous section particular the prototype provides reliable communication evaluate the cost performing the necessary flow control and error checking software minimizes the kernel intervention determine the cost providing protection software andthe buffering designed tolerate arbitrary context switching the nodes this time only limited experimental set described below available such that the prototype cannot provide information neither how cell losses due contention within the network affect performance nor how the scheduling processes can coordinated improve the overall performance parallel applications Active Messages Communication Architecture The Active Messages communication architecture offers simple general purpose communication primitives thin veneer over the raw hardware intended serve substrate for building libraries that provide higher level communication abstractions and for generating communication code directly from parallel language compiler Unlike most communication layers not intended for direct use application programmers and really provides lower level services from which communication libraries and run time systems can built The basic communication primitive message with associated small amount computation the form handler the receiving end Typically the first word Active Message points the handler for that message message arrival the computation the node interrupted and the handler executed The role the handler get the message out the network integrating into the ongoing computation and sending reply message back The buffering and scheduling provided Active Messages are extremely primitive and thereby fast the only buffering that involved actual transport and the only scheduling that required activate the handler This sufficient support many higher level abstractions and more general buffering and scheduling can easily constructed layers above Active Messages when needed This minimalist approach avoids paying performance penalty for unneeded functionality order prevent deadlock and livelock Active Message restricts communication patterns requests and replies the handler request message only allowed send reply message and reply handler not allowed send further replies SSAM functionality The current implementation geared towards the sending small messages which fit into the payload single ATM cell Eight the available bytes payload ATM cell are used SSAM hold flow control information bits the handler address bits and AAL compatible checksum bits The remaining bytes hold the Active Message data The header file for the interface SSAM shown Figure send request Active Message the user places the message data into per connection buffer provided SSAM and calls SSAM with connection identifier and the remote handler address SSAM adds the flow control information and traps the kernel have the message injected into the network also polls the receiver and processes incoming messages the receiving end the network polled SSAM SSAM poll the latter only polls the network and all messages accumulated the receive FIFO are moved into buffer SSAM then calls the appropriate handler for each message passing arguments the originating connection identifier the address the buffer holding the message and the address buffer for reply message The handler processes the message and may send reply message back placing the data the buffer provided and returning the address the reply handler NULL reply sent Figure interface for SPARCstation Active Messages The current prototype does not use interrupts instead the network polled every time message sent This means that long process sending messages will also handle incoming ones explicit polling function provided for program parts which not communicate Using interrupts planned but not implemented yet Example implementing remote read with SSAM The sample implementation split phase remote double word read shown Figure The readDouble function increments counter outstanding reads formats request Active Message with the address read well information for the reply and sends the message The readDouble handler fetches the remote location and sends reply back the readDouble reply handler which stores the data into memory and decrements the counter The originating processor waits for the completion the read busy waiting the counter the end readDouble split phase read could constructed easily exposing the counter the caller who could proceed with computation after initiating the read and only wait the counter when the data required Figure Sample remote read implementation using SSAM Experimental set The experimental set used evaluate the performance the prototype SSAM implementation consists Mhz SPARCstation and Mhz SPARCstation running SunOS The two machines are connected via Fore Systems SBA ATM interfaces using TAXI fiber The interfaces are located the Sbus bit bus running Mhz and provide cell deep output FIFO well cell input FIFO send cell the processor stores bytes into the memory mapped output FIFO and receive cell reads bytes from the input FIFO register the interface indicates the number cells available the input FIFO Note that the network interface used much simpler and closer multiprocessor NIs than most second generation ATM interfaces available today The only function performed hardware beyond simply moving cells onto off the fiber checksum generation and checking for the ATM header and AAL compatible payload particular DMA segmentation and reassembly multi cell packets provided SSAM implementation The implementation the SPARCstation ATM Active Messages layer consists two parts device driver which dynamically loaded into the kernel and user level library linked with applications using SSAM The driver implements standard functionality open and close the ATM device and provides two paths send and receive cells The fast path described here consists three trap instructions which lead directly code for sending and receiving individual ATM cells The traps are relatively generic and all functionality specific Active Messages the user level library which also performs the flow control and buffer management conventional read write system call interface provided for comparison purposes and allows send and receive cells using pure device driver approach The traps send and receive cells consist carefully crafted assembly language routines Each routine small and instructions for the send and receive traps respectively and uses available registers carefully The register usage simplified the Sparc architecture use circular register file which provides clean register window for the trap interfacing from the program the traps via function call arguments can passed and another registers become available the trap The following paragraphs describe the critical parts the SSAM implementation more detail Flow control simple sliding window flow control scheme used prevent overrun the receive buffers and detect cell losses The window size dimensioned allow close full bandwidth communication among pairs processors order implement the flow control for window size each process pre allocates memory hold cells per every other process with which communicates The algorithm send request message polls the receiver until free window slot available and then injects the cell into the network saving one the buffers well case has retransmitted Upon receipt request message the message layer moves the cell into buffer and soon the corresponding process running calls the Active Message handler the handler issues reply sent and copy held buffer the handler does not generate reply explicit acknowledgment sent Upon receipt the reply acknowledgment the buffer holding the original request message can reused Note how the distinction between requests and replies made Active Messages allows acknowledgments piggy backed onto replies The recovery scheme used case lost duplicate cells standard except that the reception duplicate request messages may indicate lost replies which have retransmitted important realize that this flow control mechanism does not really attempt minimize message losses due congestion within the network the lack flow control ATM networks effectively precludes any simple congestion avoidance scheme Until larger test beds become available and the ATM community agrees how routers should handle buffer overflows seems futile invest more sophisticated flow control mechanisms Nevertheless the bursty nature parallel computing communication patterns are likely require some solution before the performance characteristics ATM network become robust those multiprocessor networks User kernel interface and buffer management The streamlining the user kernel interface the most important factor contributing the performance SSAM the prototype the kernel preallocates all buffers for process when the device opened The pages are then pinned prevent page outs and are mapped using mmap into the processes address space After every message send the user level library chooses buffer for the next message and places pointer exported variable The application program moves the message data into that buffer and passes the connection and the handler address SSAM which finishes formatting the cell adding the flow control and handler and traps the kernel The trap passes the message offset within the buffer area and the connection registers the kernel Protection ensured with simple masks limit the connection and offset ranges lookup maps the current process and connection ids virtual circuit The kernel finally moves the cell into the output FIFO the receiving end the read ATM kernel trap delivers batch incoming cells into pre determined shared memory buffer The number cells received returned register For each cell the kernel performs four tasks loads the first half the cell into registers uses the VCI index into table obtain the address the appropriate processes input buffer moves the full cell into that buffer and checks the integrity the cell using three flag bits set the the last byte Upon return from the trap the SSAM library loops through all received cells checking the flow control information calling the appropriate handlers for request and reply messages and sending explicit acknowledgments when needed SSAM performance The following paragraphs describe performance measurements SSAM made with number synthetic benchmarks The following terminology used overhead consists the processor cycles spent preparing send receive message latency the time from which message send routine called the time the message handled the remote end and bandwidth the rate which user data transferred The performance goal for SSAM the fiber rate Mbit which transmits cell every bytes for ATM payload bandwidth ATM traps detailed cost breakdown for the operations occurring each the traps send and receive cells shown Table The two timing columns refer measurements taken the SPARCstation and the SPARCstation respectively The times have been obtained measuring repeated executions each trap with gettimeofday which uses microsecond accurate clock and takes the The time break down for each trap was measured commenting appropriate instructions out and somewhat approximate due the pipeline overlap occurring between successive instructions Table Cost breakdown for traps send and receive cells Operation write trap trap rett check pid and con nection addt kernel ovhd load cell push push cell total read trap trap rett check cell count addt kernel ovhd per cell pull from per cell demux per cell store away total for cell per cell total for cells write read trap total cells read total cell read null system call The write trap cost broken down into parts the cost the trap and return the protection checks overhead for fetching addresses loading the cell into registers and pushing the cell into the network interface The numbers show clearly that the fiber can saturated sending cell time from user level also indicates that the majority the cost lies the access the network interface across the Sbus The cost the trap itself surprisingly low even though the second largest item fact could reduced slightly the current implementation adds level indirection the trap dispatch simplify the dynamic loading the device driver The read trap itemized similarly the cost trap and return fetching the device register with the count available cells additional overhead for setting addresses loading the cell from the network interface demultiplexing among processes and storing the cell away The total cost shows trap which receives single cell well the per cell cost for trap which receives cells Here again the access the device dominates due the fact that each double word load incurs the full latency Sbus access The total time the falls short the fiber cell time and will limit the achievable bandwidth most the fiber The write read trap first sends cell and then receives chunk cells This amortizes the cost the trap across both functions and overlaps checking the cell count slightly with sending The last item the table shows the cost null system call for comparison purposes write file descriptor was used clear that system call approach would yield performance far inferior the traps and would achieve only fraction the fiber bandwidth ATM read write system calls addition the direct traps the device driver allows cells sent and received using traditional read and write system calls the device file descriptor this time this conventional path provided for comparison purposes only and the read and write entry points into the device driver are limited sending and receiving single cells Multi cell reads and writes could supported easily The read and write entry points perform the following operations check for the appropriateness the file descriptor transfer data between user space and internal buffer using uiomove andtransfer data between the internal buffer and the FIFOs the network interface The internal buffer used because the data cannot transferred directly between user space and the device using uiomove due the fact that the device FIFOs are only word addressable The use internal buffer also allows double word accesses the device FIFOs which improves the access times considerably Table shows the costs for the various parts the read and write system calls The syscall overhead entries reflect the time taken for read respectively write system call with empty read write device driver routine This measures the kernel overhead associated with these system calls The check uiomove entry reflects the time spent checking the validity the file descriptor and performing the uiomove the case read also includes the time check the device register holding the number cells available the input FIFO The push pull cell entries reflect the time spent transfer the contents one cell between the internal buffer and the device FIFOs The write and read cell totals reflect the cost the full system call while the read cells entry the time taken for unsuccessful poll which includes the system call overhead the file descriptor checks and the reading the receive ready register Table Cost sending and receiving cells using read and write system calls Operation write system call syscall overhead check uiomove push cell into write total read system call syscall overhead pull cell from check and recv ready uiomove read total for cell read total for cells The timings show clearly that the overhead the read write system call interface prohibitive for small messages For larger messages however may well viable choice and more portable than the traps SSAM Measurements the Active Messages layer built the cell send and receive traps are shown Table all cases one word the Active Message payload carries data and the handlers simply return The send request uses write read trap and adds little over overhead the for cell formatting and flow control The handling times are all roughly the cost read trap reading cells per trap plus again little over for the flow control and handler dispatch reply sent that adds the time write trap Table Cost breakdown for SPARCstation Active Messages Operation send request handle request reply sent handle request and send reply handle ack handle reply The measurements show that supporting only single cell Active Messages not optimal Longer messages are required achieve peak bulk transfer rates the one cell time prototype can yield simpler interface for shorter messages with only bytes payload might well useful well accelerate the small requests and acknowledgments that are often found higher level protocols Unfortunately given that the trap cost dominated the network interface access time and that the SBA requires all bytes cell transferred the processor unlikely that significant benefit can realized Split While full implementation Split still progress timings the remote memory access primitives show that the round trip time for remote read double word aligned bytes takes the and one way remote store takes for the same payload Remote accesses with smaller payloads are not noticeably cheaper bulk write implemented with the current SSAM layer transfers Mbytes but experiments show that using long messages this could improved Mbytes using the full ATM payload and simplifying the handling slightly Unresolved issues The current SSAM prototype has influence the kernel process scheduling Given the current buffering scheme the SSAM layer operation not influenced which process running The performance applications however likely highly influenced the scheduling How best influence the scheduler semi portable fashion requires further investigation The most promising approach appears use real time thread scheduling priorities such are available Solaris The amount memory allocated the SSAM prototype somewhat excessive and fact for simplicity the current prototype uses twice many buffers strictly necessary For example assuming that flow control window cells used the kernel allocates and pins Kbytes memory per process per connection node cluster with parallel applications running this represents memory per processor The number preallocated buffers could reduced without affecting peak bulk transfer rates adjusting the flow control window size dynamically The idea that the first cell long message contain flag which requests larger window size from the receiver few extra buffers would allocated for this purpose The receiver grants the larger window one sender time using the first acknowledgment cell the bulk transfer The larger window size remains effect until the end the long message This scheme has two benefits the request for larger window overlapped with the first few cells the long message and the receiver can prevent too many senders from transferring large data blocks simultaneously which would sub optimal for the cache However fundamentally appears that memory alternatively low performance the price pay for having neither flow control the network nor coordinated process scheduling more subtle problem having with the ATM payload alignment used the SBA interface will surface the future the bytes ATM cell are padded the SBA bytes and the byte payload starts with the byte only half word aligned The effect that bulk transfer payload formats designed with the SBA mind and supporting double word moves data between memory and the SBA will clash with other network interfaces which double word align the ATM payload Summary The prototype Active Messages implementation SPARCstation ATM cluster provides preliminary demonstration that this communication architecture developed for multiprocessors can adapted the peculiarities the workstation cluster The performance achieved roughly comparable that multiprocessor such the where the one way latency roughly but clear that without network interface closer the processor the performance gap cannot closed The time taken the flow control and protection software surprisingly low least comparison with the network interface access times The cost effect has been shifted large pre allocated and pinned buffers While the prototype memory usage somewhat excessive other schemes with comparable performance will also require large buffers Overall SSAM speed comes from careful integration all layers from the language level the kernel traps The key issues are avoiding copies having the application place the data directly where the kernel picks move into the device and passing only easy check information the kernel particular not pass arbitrary virtual address Comparison other approaches The ATM network communication layer most directly comparable SSAM the remote memory access model proposed Thekkath The implementation very similar SSAM that uses traps for reserved opcodes the MIPS instruction set implement remote read and write instructions The major difference between the two models that the remote memory operations separate data and control transfer while Active Messages unifies them With remote memory accesses data can transferred user memory the kernel without the corresponding process having run But the model used does not allow remote reads and writes the full address space process Rather each communicating process must allocate special communication memory segments which are pinned the operating system just the buffers used SSAM are The communication segments are more flexible than SSAM buffers that they can directly hold data structures limited the fact that the segments are pinned The advantage SSAM over the remote memory accesses the coupling data and control each message causes small amount user code executed which allows data scattered into complex data structures and the scheduling computation directly influenced the arrival data the remote memory access model limited control transfer offered through per segment notification flags order cause file descriptor become ready Finally SSAM provides reliable transport mechanism while the remote memory access primitives are unreliable and not provide flow control Table compares the performance the two approaches Thekkath implementation uses two DECstation interconnected Turbochannel version the same Fore ATM interface used for SSAM and performs little worse than SSAM for data transfer and significantly worse for control transfer The remote reads and writes are directly comparable that they transfer the same payload per cell Table Comparison SSAM Remote Memory Accesses between DECstation sover ATM Operation SSAM Remote mem access read latency write latency addt control none transfer ovhd block write The performance more traditional communication layers over ATM network has been evaluated Lin and shows over two orders magnitude higher communication latencies than SSAM offers Table summarizes the best round trip latencies and one way bandwidths attained Sun and SPARCstation connected Fore SBA interfaces without switch The millisecond scale reflects the costs the traditional networking architecture used these layers although not clear why Fore AAL API slower than the read write system call interface described Note that TCP implementation with well optimized fast path should yield sub millisecond latencies Table Performance traditional communication layers Sun and SPARCstation over ATM Communication layer Round trip Peak latency bandwidth Fore AAL API BSD TCP Sockets PVM over TCP Sun RPC Conclusions The emergence high bandwidth low latency networks making the use clusters workstations attractive for parallel computing style applications From technical point view continuous spectrum systems can conceived ranging from collections Ethernet based workstations tightly integrated custom multiprocessors However this paper argues that clusters will characterized the use off the shelf components which will handicap them with respect multiprocessors which hardware and software are customized allow tighter integration the network into the overall architecture The use standard components and particular ATM networking technology results three major disadvantages clusters with respect multiprocessors ATM networks not offer reliable delivery flow control the current network interfaces are not well integrated into the workstation architecture and iii the operating systems the nodes cluster not coordinate process scheduling address translations The prototype implementation the Active Messages communication model described this paper achieves two orders magnitude better performance than traditional networking layers Table shows that the resulting communication latencies and bandwidths are the same ball park state the art multiprocessors Key the success are the use large memory buffers and the careful design lean user kernel interface The major obstacle towards closing the remaining performance gap the slow access the network interface across the bus and reducing the buffer memory usage requires coordination process scheduling across nodes While taking care flow control software does not dominate performance this study the behavior ATM networks under parallel computing communication loads remains open question Table Comparison SSAM performance with that recent parallel machines Machine Peak Round trip bandwidth latency MPL Paragon Active Mesg cluster SSAM Bibliography CCITT Recommendation ISDN ATM functional characteristics Revised version Geneva ITU Culler Dusseau Goldstein Krishnamurthy Lumetta von Eicken and Yelick Introduction Split Proc Supercomputing Culler Dusseau Martin Schauser Fast Parallel Sorting from LogP Split Proc WPPP July von Eicken Culler Goldstein and Schauser Active Messages Mechanism for Integrated Communication and Computation Proc the ISCA pages May Geist Beguelin Dongarra Jiang Manchek and Sunderam PVM User Guide and Reference Manual Oak Ridge National Laboratory Technical Report ORNL February and Hudak Memory Coherence Shared Virtual Memory Systems ACM Transactions Computer Systems November Lin Hsieh Thomas and MacDonald Distributed Network Computing over Local ATM Networks IEEE Journal Selected Areas Communications Special Issue ATM LANs appear Pierce and Regnier The Paragon Implementation the Message Passing Interface Proc SHPCC May Stunkel Shea Grice Hochschild and Tsao The High Performance Switch Proc SHPCC May Thekkath Levy and Lazowska Efficient Support for Multicomputing ATM Networks University Washington Technical Report April Thekkath Levy and Lazowska Separating Data and Control Transfer Distributed Operating Systems Proc the Int Conf ASPLOS appear October Thinking Machines Corporation Cambridge Massachusetts Connection Machine Technical Summary November Footnotes The term cluster used here refer collections workstation class machines interconnected low latency high bandwidth network This paper focuses exclusively scalable multiprocessor architectures and specifically excludes bus based shared memory multiprocessors Current ATM switches have latencies about order magnitude higher than comparable multiprocessor networks however this difference does not seem inherent ATM networks least not for local area switches discussion differences fault isolation characteristics beyond the scope this paper Although some transmission media may cause burst errors which are beyond the correction capabilities most CRC codes Cache coherent shared memory stretch this characterization given that the cache the receiving node essentially performs another address translation which may miss and require additional communication with other nodes complete the request All bandwidths are measured megabytes per second The kernel write protects the trap vectors after boot The SSAM prototype uses permanently loaded trap which performs indirect jump via kernel variable allow simple dynamic driver loading Note that more realistic setting Fore ASX switch will add roughly latency the write time and the round trip read time One could easily describe the traps employed SSAM additional emulated communication instructions Date Tue Jan GMT Server NCSA Content type text html Former Students Bob BoyerFormer Students Bob Boyer William Bevier Now CLInc Shang Ching ChouNow Wichita State UniversityJ Christian Arthur Flatau Now CLInc Goldschlag Warren Hunt Now CLInc Kim Nick McPheeNow The University MinnesotaNatarajan ShankarNow SRIS Subramanian William Young Now CLInc you have feedback want more information Contact Benjamin Shults Date Tue Jan GMT Server NCSA Content type text html Last modified Fri Nov GMT Content length The Department Computer Science Help Using WWW Creating your home page Information html language Usage Statistics for www pitt edu server 