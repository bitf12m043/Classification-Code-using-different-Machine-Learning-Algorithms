MIME Version Server CERN Date Sunday Dec GMT Content Type text html Content Length Last Modified Monday Mar GMT Resume References for Nicholas Howe Resume ReferencesResume PostScript References PostScript MIME Version Server CERN Date Tuesday Jan GMT Content Type text html Content Length Last Modified Wednesday Dec GMT SSGRG Project PageProject IndexThis web page presents brief description each SSGRG project and comprehensive list papers relevant that project The project index PredatorADAGEP GenVocaRosettaDissertationsPrairieDesign PatternsMiscellaneousIf you are interested specific topics subjects perform keyword search the SSGRG Publication Page Related Web Pages UTCS General SSGRG Title Page SSGRG Publication PageP Predator series prototypes for GenVoca generator container data structures Memory Simulators and Software Generators SSR Composition Validation and Subjectivity IEEE TSE Subjectivity and GenVoca Generators ICSR Lightweight DBMS Generator UTCS Extensible Lightweight DBMS UTCS Validating Component Compositions ICSR Reengineering Complex Application ACM SIGSOFT The LEAPS Algorithms UTCS Introductory System Manual PostScript file UTCS Advanced System Manual PostScript file UTCS Scalable Software Libraries ACM SIGSOFT Scalable Approach Software Libraries WISR Software Components Data Structure Precompiler ICSE Single Schema Database Management Systems UTCS Implementing Domain Model for Data Structures IJSEKE Software Distribution tar file Manuals Distribution tar file extension that supports GenVoca programming concepts Programming Language for Generators Thesis Language for Large Scale Reusable Software WISR Language for Software System Generators Rosetta project build extensible and customized compilers for the domain relational data languages Rosetta Generator Data Language Compilers SSR Generation Extensible Data Languages Thesis Prairieis effort build customized rule based query optimizers from components Synthesizing Rule Sets UTCS Prairie Rule Specification Framework Data Engineering Making Database Optimizers More Extensible Thesis Design Patterns Many structural Gamma patterns can recognized end products large scale program transformations Our work design patterns explores the potential automating structural patterns for software maintenance Program Transformations for Evolving Software OOPSLA Automated Software Evolution via Design Patterns ISACC ADAGE ARPA sponsored DSSA project build GenVoca generator for avionics software Creating Reference Architectures SSR The ADAGE Avionics Reference Architecture CIA Domain Modeling Symposium ECBS GenVoca Composition Validation and Subjectivity IEEE TSE Rosetta Generator Data Language Compilers SSR Generators Transformation Systems and Compilers working paper Subjectivity and GenVoca Generators ICSR Issues Domain Modeling OOPSLA Validating Component Compositions ICSR The GenVoca Model IEEE Software September Products Domain Models Workshop The Design and Implementation ACM TOSEM October Dissertations Programming Language for Generators Singhal Making Database Optimizers More Extensible Das Generation Extensible Data Languages Villarreal Generator for Data Structures Sirkin Miscellaneous Component Technologies and Space Applications Integ Micro Last modified December Don Batory batory utexas edu MIME Version Server CERN Date Tuesday Jan GMT Content Type text html Content Length Last Modified Tuesday Dec GMT Course Notes info net introduction lecture postscript RSA lecture html notes for homework html From Date Tue Nov GMT Server NCSA Content type text html Last modified Thu Oct GMT Content length Memory Management Lecture NotesMemory Management Contents Allocating Main Memory Algorithms for Memory Management Compaction and Garbage Collection Swapping Allocating Main Memory first consider how manage main core memory also called random access memory RAM general memory manager provides two operations Address allocate int size void deallocate Address block The procedure allocate receives request for contiguous block size bytes memory and returns pointer such block The procedure deallocate releases the indicated block returning the free pool for reuse Sometimes third procedure also provided Address reallocate Address block int new size which takes allocated block and changes its size either returning part the free pool extending larger block may not always possible grow the block without copying new location reallocate returns the new address the block Memory allocators are used variety situations Unix each process has data segment There system call make the data segment bigger but system call make smaller Also the system call quite expensive Therefore there are library procedures called malloc free and realloc manage this space Only when malloc realloc runs out space necessary make the system call The operators new and delete are just dressed versions malloc and free The Java operator new also uses malloc and the Java runtime sustem calls free when object found inaccessible during garbage collection described below The operating system also uses memory allocator manage space used for data structures and given user processes for their own use saw before there are several reasons why might want multiple processes such serving multiple interactive users controlling multiple devices There also selfish reason why the wants have multiple processes memory the same time keep the CPU busy Suppose there are processes memory this called the level multiprogramming and each process blocked waiting for fraction the time the best case when they take turns being blocked the CPU will busy provided For example each process ready the time and the CPU could kept completely busy with five processes course real processes aren cooperative the worst case they could all decide block the same time which case the CPU utilization fraction the time the CPU busy would only our example each processes decides randomly and independently when block the chance that all processes are blocked the same time only CPU utilization Continuing our example which and the expected utilization would other words the CPU would busy about the time the average See Figure page the text Algorithms for Memory Management Clients the memory mangager keep track allocated blocks for now will not worry about what happens when client forgets about block The memory manager needs keep track the holes between them The most common data structure doubly linked list holes sorted address This data structure called the free list This free list doesn actually consume any space other than the head and tail pointers since the links between holes can stored the holes themselves provided each hole least large two pointers satisfy allocate request the memory manager finds hole size least and removes from the list the hole bigger than bytes can split off the tail the hole making smaller hole which returns the list satisfy deallocate request the memory manager turns the returned block into hole data structure and inserts into the appropriate place the free list the new hole immediately preceded followed hole the holes can coalesced into bigger hole How does the memory manager know how big the returned block The usual trick put small header the allocated block containing the size the block and perhaps some other information The allocate routine returns pointer the body the block not the header the client doesn need know about The deallocate routine subtracts the header size from its argument get the address the header The client thinks the block little smaller than really long the client colors inside the lines there problem but the client has bugs and scribbles the header the memory manager can get completely confused This frequent problem with malloc Unix programs written The Java system uses variety runtime checks prevent this kind bug How does the memory manager choose hole respond allocate request first might seem that should choose the smallest hole that big enough satisfy the request This strategy called best fit has two problems First requires expensive search the entire free list find the best hole although fancier data structures can used speed the search More importantly leads the creation lots little holes that are not big enough satisfy any requests This situation called fragmentation and problem for all memory management strategies although particularly bad for best fit One way aviod making little holes give the client bigger block than asked for For example might round all requests the next larger multiple bytes That doesn make the fragmentation away just hides Unusable space the form holes called external fragmentation while unused space inside allocated blocks called internal fragmentation Another strategy first fit which simply scans the free list until large enough hole found Despite the name first fit generally better than best fit because leads less fragmentation There still one problem Small holes tend accumulate near the beginning the free list making the memory allocator search farther and farther each time This problem solved with next fit which starts each search where the last one left off wrapping around the beginng when the end the list reached Yet another strategy maintain separate lists each containing holes different size Tanenbaum multiprogramming with fixed partitions Section page can viewed extreme case this approach which the number holes very small This approach works well the application level when only few different types objects are created although there might lots instances each type can also used more general setting rounding all requests one few pre determined choices For example the memory mangager may round all requests the next power two bytes with minimum say and then keep lists holes size etc Assuming the largest request possible megabyte this requires only lists This the approach taken most implementations malloc This approach eliminates external fragmentation entirely but internal fragmenation may bad the worst case which occurs when all requests are one byte more than power two Another problem with this approach how coalesce neighboring holes One possibility not try The system initialized splitting memory into fixed set holes either all the same size variety sizes Each request matched appropriate hole the request smaller than the hole size the entire hole allocated anyhow When the allocate block released simply returned the appropriate free list Most implementations malloc use variant this approach some implementations split holes but most never coalesce them interesting trick for coalescing holes with multiple free lists the buddy system Assume all blocks and holes have sizes which are powers two requests are always rounded the next power two and each block hole starts address that exact multiple its size Then each block has buddy the same size adjacent such that combining block size with its buddy creates properly aligned block size For example blocks size could start addresses etc The blocks and are buddies combining them gives block length Similarly and are buddies and are buddies etc The blocks and are not buddies even though they are neighbors Combining them would give block size starting address which not multiple The address block buddy can easily calculated flipping the nth bit from the right the binary representation the block address For example the pairs buddies binary are each case the two addresses the pair differ only the third bit from the right short you can find the address the buddy block taking the exclusive the address the block with its size allocate block given size first round the size the next power two and look the list blocks that size that list empty split block from the next higher list that list empty first add two blocks splitting block from the next higher list and When deallocating block first check see whether the block buddy free combine the block with its buddy and add the resulting block the next higher free list with allocations deallocations can cascade higher and higher lists Compaction and Garbage Collection What you when you run out memory Any these methods can fail because all the memory allocated because there too much fragmentation Malloc which being used allocate the data segment Unix process just gives and calls the expensive call expand the data segment memory manager allocating real physical memory doesn have that luxury The allocation attempt simply fails There are two ways delaying this catastrophe compaction and garbage collection Compaction attacks the problem fragmentation moving all the allocated blocks one end memory thus combining all the holes Aside from the obvious cost all that copying there important limitation compaction Any pointers block need updated when the block moved Unless possible find all such pointers compaction not possible Pointers can stored the allocated blocks themselves well other places the client the memory manager some situations pointers can point not only the start blocks but also into their bodies For example block contains executable code branch instruction might pointer another location the same block Compaction performed three phases First the new location each block calculated determine the distance the block will moved Then each pointer updated adding the amount that the block pointing will moved Finally the data actually moved There are various clever tricks possible combine these operations Garbage collection finds blocks memory that are inaccessible and returns them the free list with compaction garbage collection normally assumes find all pointers blocks both within the blocks themselves and from the outside that not possible can still conservative garbage collection which every word memory that contains value that appears pointer treated pointer The conservative approach may fail collect blocks that are garbage but will never mistakenly collect accessible blocks There are three main approaches garbage collection reference counting mark and sweep and generational algorithms Reference counting keeps each block count the number pointers the block When the count drops zero the block may freed This approach only practical situations where there some higher level software keep track the counts much too hard hand and even then will not detect cyclic structures garbage Consider cycle blocks each which only pointed its predecessor the cycle Each block has reference count but the entire cycle garbage Mark and sweep works two passes First mark all non garbage blocks doing depth first search starting with each pointer from outside void mark Address mark block for each pointer block the block pointed not marked mark The second pass sweeps through all blocks and returns the unmarked ones the free list The sweep pass usually also does compaction decribed above There are two problems with mark and sweep First the amount work the mark pass proportional the amount non garbage Thus memory nearly full will lot work with very little payoff Second the mark phase does lot jumping around memory which bad for virtual memory systems will soon see The third approach garbage collection called generational collection Memory divided into spaces When space chosen for garbage collection all subsequent references objects that space cause the object copied new space After while the old space either becomes empty and can returned the free list all once least becomes sparse that mark and sweep garbage collection will cheap empirical fact objects tend either short lived long lived other words object that has survived for while likely live lot longer carefully choosing where move objects when they are referenced can arrange have some spaces filled only with long lived objects which are very unlikely become garbage garbage collect these spaces seldom ever Swapping When all else fails allocate simply fails the case application program may adequate simply print error message and exit must able recover more gracefully motivated memory management the desire have many processes memory once batch system the cannot allocate memory start new job can recover simply delaying starting the job there queue jobs waiting created the might want down the list looking for smaller job that can created right away This approach maximizes utilization memory but can starve large jobs The situation analogous short term CPU scheduling which SJF gives optimal CPU utilization but can starve long bursts The same trick works here aging job waits longer and longer increase its priority until its priority high that the refuses skip over looking for more recently arrived but smaller job alterantive way avoiding starvation use memory allocation scheme with fixed partitions holes are not split combined Assuming job bigger than the biggest partion there will starvation provided that each time partition freed start the first job line that smaller than that partition However have another choice analogous the difference between first fit and best fit course want use the best hole for each job the smallest free partition that least big the job but suppose the next job line small and all the small partitions are currently use might want delay starting that job and look through the arrival queue for job that better uses the partitions currently available This policy introduces the possibility starvation which can combat aging above disk available can also swap blocked jobs out disk When job finishes first swap back jobs form disk before allowing new jobs start When job blocked either because wants because our short term scheduling algorithm says switch another job have choice leaving memory swapping out One way looking this scheme that increases the multiprogramming level the number jobs memory the cost making much more expensive switch jobs variant the MLFQ multi level feedback queues CPU scheduling algorithm particularly attractive for this situation The queues are numbered from some maximum When job becomes ready enters queue zero The CPU scheduler always runs job from the lowest numbered non empty queue the priority the negative the queue number runs job from queue for maximum quanta the job does not block complete within that time limit added the next higher queue This algorithm behaves like with short quata that short bursts get high priority but does not incur the overhead frequent swaps between jobs with long bursts The number swaps limited the logarithm the burst size solomon wisc edu Thu Oct CST Copyright Marvin Solomon All rights reserved 