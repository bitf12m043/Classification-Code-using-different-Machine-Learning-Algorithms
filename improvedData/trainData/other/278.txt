MIME Version Server CERN Date Sunday Dec GMT Content Type text html Content Length Last Modified Saturday Dec GMT PyraMania Home PagePyraMania Space AdventureIntroduction PyraMania Spaceship battle game developed project for the course Cornell University Jose Luis Fernandez Samuel Kan Todd Peskin Amith Yamasani wireframe simulation with options display solid objects Programming Languages Utilized PyraMania was developed using for the internal structure the program and TCL for the user interface Features Wireframe graphics solid graphics mode Radar for locating enemies and tanks Lasser Objective The user must get all the rotating combustible tanks the scenary has radar locate the enemies and tanks and lasser shoot the enemy spaceships must complete the mission before the timer counter will expire MIME Version Server CERN Date Tuesday Jan GMT Content Type text html Content Length Last Modified Friday Jun GMT OOPS Group PublicationsOOPS Group Publications Sheetal Kakkad Mark Johnstone and Paul Wilson Portable Runtime Type Description for Conventional Compilers Submitted USENIX Many useful language extensions and support libraries require knowledge the layout fields within objects runtime Examples include orthogonal persistent object stores garbage collectors data structure picklers parameter marshalling schemes etc For clean and efficient implementation libraries these systems require knowledge the actual layouts data objects which unavailable most traditionally compiled and linked programming languages such and Ada present facility for runtime type description RTTD which extracts the low level layout information from debug output conventional compilers and makes available user programs describe the basic strategies the system and present details our interface for also sketch some extensions have implemented including special treatment virtual function table pointers Our implementation freely available via anonymous ftp Postscript Michael Neely Analysis the Effects Memory Allocation Policy Storage Fragmentation Master thesis The University Texas Austin May The study dynamic memory allocation has been dominated measurement performance allocators with random input streams requests This study introduces new methodology that separates the issue policy from implementation and focuses the effects placement policy fragmentation studies the effects policy decisions such object placement splitting and coalescing storage fragmentation useful and accurate measurement fragmentation presented that based the amount waste the point peak memory usage have attempted pick representative set allocators from the space reasonable combinations known policies The allocators are used memory allocation simulations determine their respective fragmentation Our results show that the best fit FIFO LIFO address ordered and address ordered first fit policies yield the lowest fragmentation average and that the overheads associated with these allocators are the largest source wasted memory also explain how best fit can implemented efficiently representative set real program allocation traces used the simulations and compared with randomized traces show that the application patterns allocation are important factor the allocator performance and that studies based synthetic traces are fundamentally flawed Compressed Postscript Paul Wilson Mark Johnstone Michael Neely and David Boles Dynamic Storage Allocation Survey and Critical Review International Workshop Memory Management Kinross Scotland September Dynamic memory allocation has been fundamental part most computer systems since roughly and memory allocation widely considered either solved problem insoluble one this survey describe variety memory allocator designs and point out issues relevant their design and evaluation then chronologically survey most the literature allocators between and Scores papers are discussed varying detail and over references are given argue that allocator designs have been unduly restricted emphasis mechanism rather than policy while the latter more important higher level strategic issues are still more important but have not been given much attention Most theoretical analyses and empirical allocator evaluations date have relied very strong assumptions randomness and independence but real program behavior exhibits important regularities that must exploited allocators are perform well practice Postscript Paul Wilson Sheetal Kakkad and Shubhendu Mukherjee Anomalies and Adaptation the Analysis and Development Prefetching Policies Journal Systems and Software November Technical communication Analysis and Development Demand Prepaging Policies Horspool and Huberman show that possible design prefetching memory policies that preserve stack inclusion property much like LRU allowing them simulate these policies for all sizes memory single pass through reference trace believe that the details Horspool and Huberman algorithms introduce unexpected anomalous properties however particular their policies are not properly timescale relative events occuring timescale that should only matter some sizes memory adversely affect replacement decisions for memories very different sizes Slight changes the algorithms can restore timescale relativity and make them much better behaved addition would like point out that Horspool and Huberman algorithms actually simulate adaptive policies which may explain some their unexpectedly positive results This view suggests that properly timescale relative inclusion preserving policies can used systematically evaluate adaptive memory management schemes Postscript Paul Wilson and Mark Johnstone Real Time Non Copying Garbage Collection Position paper for the ACM OOPSLA Workshop Memory Management and Garbage Collection Washington September Postscript Paul Wilson Uniprocessor Garbage Collection Techniques International Workshop Memory Management Malo France September The Proceedings has been published Springer Verlag Lecture Notes Computer Science survey basic garbage collection algorithms and variations such incremental and generational collection The basic algorithms include reference counting mark sweep mark compact copying and treadmill collection Incremental techniques can keep garbage collection pause times short interleaving small amounts collection work with program execution Generational schemes improve efficiency and locality garbage collecting smaller area more often while exploiting typical lifetime characteristics avoid undue overhead from long lived objects Postscript Paul Wilson Uniprocessor Garbage Collection Techniques Draft much expanded version the above paper revision accepted for ACM Computing Surveys survery basic garbage collection algorithms and variations such incremental and generational collection then discuss low level implementation considerations and relationships between storage management systems languages and compilers Throughout attempt present unified view based abstract traversal strategies addressing issues conservatism opportunism and immediacy reclamation also point out variety implemetation details that are likely have significant impact the performance Postscript Paul Wilson and Sheetal Kakkad Pointer Swizzling Page Fault Time Efficiently and Compatibly Supporting Huge Address Spaces Standard Hardware International Workshop Object Orientation Operating Systems pages Paris France September Pointer swizzling page fault time novel address translation mechanism that exploits conventional address translation hardware can support huge address spaces efficiently without long hardware addresses such large address spaces are attractive for persistent object stores distributed shared memories and shared address space operating systems This swizzling scheme can used provide data compatibility across machines with different word sizes and even provide binary code compatibility across machines with different hardware address sizes Pointers are translated swizzled from long format shorter hardware supported format page fault time extra hardware required and continual software overhead incurred presence checks indirection pointers This pagewise technique exploits temporal and spatial locality much the same way normal virtual memory this gives many desirable performance characteristics especially given the trend toward larger main memories easy implement using common compilers and operating systems Postscript Vivek Singhal Sheetal Kakkad and Paul Wilson Texas Efficient Portable Persistent Store Persistent Object Systems Proceedings the Fifth International Workshop Persistent Object Systems pages San Miniato Italy September Texas persistent storage system for providing high performance while emphasizing simplicity modularity and portability key component the design the use pointer swizzling page fault time which exploits existing virtual memory features implement large address spaces efficiently stock hardware with little change existing compilers Long pointers are used implement enormous address space but are transparently converted the hardware supported pointer format when pages are loaded into virtual memory Runtime type descriptors and slightly modified heap allocation routines support pagewise pointer swizzling allowing objects and their pointer fields identified within pages compiler support for runtime type identification not available simple preprocessor can used generate type descriptors This address translation largely independent issues data caching sharing and checkpointing employs operating systems existing virtual memories for caching and simple and flexible log structured storage manager improve checkpointing performance Pagewise virtual memory protections are also used detect writes for logging purposes without requiring any changes compiled code This may degrade checkpointing performance for small transactions with poor locality writes but page diffing and sub page logging promise keep performance competitive with finer grained checkpointing schemes Texas presents simple programming interface application creates persistent object simply allocating them the persistent heap addition the implementation relatively small and easy incorporate into existing applications The log structured storage module easily supports advanced extensions such compressed storage versioning and adaptive reorganization Postscript Paul Wilson Michael Lam and Thomas Moher Caching Considerations for Generational Garbage Collection ACM Symposium Lisp and Functional Programming San Francisco California June Postscript More papers bibliography heap management and the source code for Texas Persistent Store are available via anonymous ftp ftp utexas edu pub garbage The README file lists all the available material including subdirectories which contain collected papers from the and OOPSLA Garbage Collection and Memory Management Workshops Sheetal Kakkad Date Tue Jan GMT Server NCSA Last modified Thu Nov GMT Content type text html Content length Fortran Parallel Programming SystemsFortran Parallel Programming Systems The Fortran Parallel Programming Systems Fortran Tools project seeks make parallel computer systems truly usable for Fortran programmers this effort special emphasis placed data parallel programming and scalable parallelism This Page Project Overview Fortran Language Compilers High Performance Fortran HPF Irregular Problems The System Related Projects PeopleProject Home Pages ADIFOR The System ParaScopeRelated Links Rice University home page Rice Compiler Group home page Rice Computer Science home page Center for Research Parallel Computation CRPC home pageProject Overview The Fortran Tools effort seeks make parallel computer systems usable for Fortran programmers this effort special emphasis placed data parallel programming and scalable parallelism achieve this goal researchers are developing coordinated programming system that includes compilers and tools for Fortran extended dialect Fortran that supports machine independent data parallel programming The tools support variety parallel programming activities including intelligent editing and program transformation parallel debugging performance estimation performance visualization and tuning and automatic data partitioning Research efforts also include validation the compilers and tools realistic applications well investigations new functionality handle irregular computations parallel and automatic differentiation using the program analysis infrastructure developed for the project Fortran Language and Compilers Existing languages for parallel programming scalable parallel systems are primitive and hard use They are primitive the sense that each one reflects the architecture the target machine for which intended making programs written for current parallel systems highly machine dependent result there protection the programming investment parallel machines program written for one target machine may need completely rewritten when the next generation machine available This situation the principal impediment widespread use scalable parallel systems for science and engineering problems address this problem researchers have developed Fortran set extensions Fortran and Fortran that permit the programmer specify machine independent way how distribute program principal data structures among the processors parallel system addition Fortran makes programming easier than with explicit message passing because programmers can write codes that use shared name space independent the target architecture Programmers find shared name space easier use than distributed name space because data placement and access issues can ignored Using sophisticated compiler techniques these high level programs can compiled for both SIMD and MIMD parallel architectures The Fortran research effort has led prototype compilers for the Intel Paragon and Thinking Machines for both Fortran and Fortran addition the Fortran compiler has been ported number other machines including the Intel Paragon nCube and network workstations Compilers for other machines such the SIMD MasPar are under development The strategy for all these compilers based upon deep program analysis aggressive communication optimization advanced code generation techniques and the use sophisticated computation and communication libraries The effectiveness these methods being evaluated using suite scientific programs developed affiliated researchers Syracuse University High Performance Fortran HPF Fortran was major impetus behind the definition High Performance Fortran HPF The High Performance Fortran Forum which produced the definition HPF includes representatives from industry academia and government laboratories The Fortran compilers produced part the Rice Compiler Group are being used models for several commercial HPF compilers Thus the project has established efficient technology transfer mechanism which new features Fortran once demonstrated may included future round HPF definition HPF home page Irregular Problems The Fortran group also works closely with applications scientists and engineers working irregular scientific problems such computational fluid dynamics computational chemistry computational biology structural mechanics and electrical power grid calculations Key aspects the research associated with irregular scientific problems focuses the development portable runtime support libraries which coordinate interprocessor data movement manage the storage and access copies off processor data support shared name space and couple runtime data and workload partitioners compilers These runtime support libraries are being used port application codes variety multiprocessor architectures and are being incorporated into the Fortran distributed memory compilers For list technical papers irregular problems see the System Technical Papers Web pages The System researchers Rice University set out under ARPA funding build suite prototype tools that support development programs Fortran abstract machine independent parallel programming language The tools emerging from this research are collectively being called the System date research has focused four key areas intelligent editor for Fortran that provides feedback the analysis and parallelization performed Fortran compiler developed Rice advances automatic data distribution research performed Rice joint work performance analysis Fortran programs with collaborators the University Illinois and overhaul and integration program analysis repository with the interprocedural analysis system provide firm basis for the development efficient whole program analysis tools System home page The System grew out collection tools called ParaScope which was initially designed support development Fortran programs with explicit parallelism the form parallel loops One difference between ParaScope and the System that ParaScope focuses shared memory machines whereas the System targeted distributed shared memory DSM machines ParaScope home page Related Projects Members the Fortran group are involved several additional collaborations that are capitalizing the available software infrastructure For instance researchers Rice University and Argonne National Laboratory are continuing enhance ADIFOR automatic differentiation tool for Fortran built upon the ParaScope infrastructure support sensitivity analysis large simulation codes for use multidisciplinary design optimization members the CRPC Parallel Optimization group ADIFOR home page The Massively Scalar Compiler Project MSCP Rice exploiting the interprocedural analysis engine developed for ParaScope and also investigating interactions between parallelizing transformations and scalar node performance MSCP home page The Fortran group also collaborating with the CRPC Parallel Paradigm Integration project investigate ways integrating ParaScope and Fortran style data decomposition directives into Fortran modular version Fortran Syracuse University coordinating ARPA activity set Parallel Compiler Runtime Consortium This involves other sites and aims design and implement common runtime support for parallel Fortran and Ada for both data and task parallelism Finally Rice University active collaborator project the Intel Delta Consortium develop software support for parallel The Fortran project researchers will develop and implement extensions Fortran that support out core arrays which are too large fit into the main memory even massively parallel computer system People Present members the Fortran Tools effort include Vikram Adve Alan Carle Keith Cooper Ken Kennedy Charles Koelbel John Mellor Crummey Linda Torczon and Scott Warren meet the people behind specific Fortran Tools project see the home page for that project ADIFOR The System ParaScope Rice University Computer Science Department Rice Compiler Group Center for Research Parallel Computation CRPC Rice University Updated Debbie Campbell dcamp rice edu http www crpc rice edu fortran tools fortran tools html Date Tue Nov GMT Server NCSA Content type text html Last modified Sat Sep GMT Content length syllabusMW htmlSyllabus for Machine Organization and ProgrammingSection Lecturer Jerry Tutsch Fall Week Day Page Chapter Topic ToDo Abstractions Computers SAL Simple Abstract Language SAL Simple Abstract Language SAL Number Systems SAL simple proc calls due Number Systems due Data Representations Data Representations base conversions complement Arithmetic Logic Ops SAL base conversions due Arithmetic Logic Ops due Floating Point Arithmetic SAL arrays due Floating Point Arithmetic due logic ops shifts IEEE FPS Data Structures Data Structures due Registers and MAL SAL stack due Registers and MAL stacks queues MAL Procedures MAL system stack due Procedures The Assembly Process The Assembly Process due procs ARs code generation Input Output MAL recursion ARs due Input Output Interrupts Exception Handling Interrupts Exception Handling due memio kernel Architectural Performance Architectural Performance Architectural Performance MAL memory mapped due Alternative Architectures pipelining cache Alternative Architectures Review for Final Thurs Final Exam comprehensive place TBD 