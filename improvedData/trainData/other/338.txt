MIME Version Server CERN Date Sunday Nov GMT Content Type text html Content Length Last Modified Friday Dec GMT WELCOME GRID GENERATION PROJECT CORNELL GRID GENERATION PROJECT CORNELL Paul Chew Nikos Chrisochoides and Steve Vavasis Research Objectives Mathematically Guaranted Quality Quadree and Octree based approach Delaunay Triangulation based approach Robust Minimize human interaction Parallel Grid Generation Applications For more information contact MIME Version Server CERN Date Monday Jan GMT Content Type text html Content Length Last Modified Thursday Dec GMT Information about Scott Well can say there lot info put here but promise put some more later student the The University Texas Austin the program for Computer Science what the heck here info how reach Scott Kaplan Taylor Hall Austin Home phone Work phone Cell phone you like find person your best bet check Taylor Hall But don keep much the way regular hours would just luck Good bad leave that you Okay this almost useless page but there will more some point you feel like email back Scott home page MIME Version Server CERN Date Tuesday Jan GMT Content Type text html Content Length Last Modified Wednesday Jan GMT Suggestions for ProjectsSuggestions for Projects are collecting number possible projects here for students pick from While you are encouraged dream your own you can also pick one off this list For lack better systemization have grouped the projects into those which are primarily implementation write code for small interactive application and those which are primarily theory oriented study the smoothness class subdivision schemes The latter may times involve small amounts implementation along the lines writing Maple program manipulate various Fourier transform expressions subdivision matrices for example Note that his list incomplete and are adding along Implementation ProjectsLoop scheme Loop scheme subdivision method for surfaces which will work for arbitrary triangular meshes and which generalizes quartic triangular splines The main references this are Loop thesis from Utah which have hardcopy only and recent Siggraph paper Piecewise Smooth Surface Reconstruction Hugues Hoppe Siggraph pages Peters surface scheme Date Tue Nov GMT Server NCSA Content type text html Last modified Thu Oct GMT Content length Processes Part III Implementation Lecture NotesProcesses and Synchronization Part III Implementation Processes Contents Implementing Monitors Implementing Semaphores Implementing Critical Sections Short term Scheduling Implementing Processes presented processes from the user point view bottom starting with the process concept then introducing semaphores way synchronizing processes and finally adding higher level synchronization facility the form monitors will now explain how implement these things the opposite order starting with monitors and finishing with the mechanism for making processes run Tanenbaum makes big deal out showing that various synchronization primitives are equivalent each other Section While this true kind misses the point easy implement semaphores with monitors you saw the first part Project but that not the way usually works Normally semaphores something very like them are implemented using lower level facilities and then they are used implement monitors Implementing Monitors Assuming that all have semaphores and would rather have monitors will assume that our semaphores have extra operation beyond the the standards operations and down semaphore awaited returns true any processes are currently waiting for the semaphore This feature not normally provided with semaphores because race condition limits its usefulness the time awaited returns true some other process may have done making false happens that this not problem for the way will use semaphores implement monitors Since monitors are language feature they are implemented with the help compiler response the keywords monitor condition signal wait and notify the compiler inserts little bits code here and there the program will not worry about how the compiler manages that but only concern ourselves with what the code and how works The monitor keyword says that there should mutual exclusion between the methods the monitor class the effect similar making every method synchronized method Java Thus the compiler creates semaphore mutex initialized and adds muxtex down the head each method also adds chunk code that call exit described below each place where method may return the end the procedure each return statement each point where exception may thrown each place where goto might leave the procedure the language has gotos etc Finding all these return points can tricky complicated procedures which why want the compiler help out When process signals notifies condition variable which some other process waiting have problem can both the processes immediately continue since that would violate the cardinal rule that there may never more than one process active methods the same monitor object the same time Thus must block one the processes the signaller the case signal and the waiter the case notify call this semaphore highPriority since processes blocked are given preference over processes blocked mutex trying get form the outside The highPriority semaphore initialized zero Each condition variable becomes semaphore sem initialized zero and wait becomes highPriority awaited highPriority else mutex sem down Before process blocks condition variable lets some other process ahead preferably one waiting the highPriority semaphore The operation signal becomes sem awaited sem highPriority down Notice that signal condition that not awaited has effect and that signal condition that awaited immediately blocks the signaller Finally the code for exit which placed every return point highPriority awaited highPriority else mutex Note that this the same code for wait except for the final sem down systems that use notify such Java notify replaced sem awaited sem these systems the code for wait also has modified wait the highPriority semaphore after getting the semaphore associated with the condition highPriority awaited highPriority else mutex sem down highPriority down system offers both signal and notify This generic implementation monitors can optimized special cases First note that process that exits the monitor immediately after doing signal need not wait the highPriority semaphore This turns out very common occurrence worth optimizing for this special case signal only allowed just before return the implementation can further simplified See Fig page Tanenbaum Finally note that not use the full generality semaphores this implementation monitors The semaphore mutex only takes the values and called binary semaphore and the other semaphores never have any value other than zero Implementing Semaphores simple minded attempt implement semaphores might look like this class Semaphore private int value Semaphore int value public void down while value value public void value There are two things wrong with this solution First have seen before attempts manipulate shared variable without synchronization can lead incorrect results even the manipulation simple value had monitors could make the modifications value atomic making the class into monitor making each method synchronized but remember that monitors are implemented with semaphores have implement semaphores with something even more primitive For now will assume that have critical sections bracket section code with begin and end begin something end the code will execute atomically were protected semaphore mutex down something mutex where mutex semaphore initialized course can actually use semaphore implement semaphores will show how implement begin and end the next section The other problem with our implementation semaphores that includes busy wait While Semaphore down waiting for value become non zero looping continuously testing the value Even the waiting process running its own CPU this busy waiting may slow down other processes since repeatedly accessing shared memory thus interfering with accesses that memory other CPU shared memory unit can only respond one CPU time there only one CPU the problem even worse Because the process calling down running another process that wants call may not get chance run What need some way put process sleep had semaphores could use semaphore but once again need something more primitive For now let assume that there data structure called PCB short for Process Control Block that contains information about process and procedure swap process that takes pointer PCB argument When swap process pcb called state the currently running process the one that called swap process saved pcb and the CPU starts running the process whose state was previously stored pcb instead Given begin end and swap process the complete implementation semaphores quite simple but very subtle class Semaphore private PCB queue waiters processes waiting for this Semaphore private int value negative number waiters static PCB queue ready list list all processes ready run Semaphore int value public void down begin value value The current process must wait Find some other process run The ready list must non empty there global deadlock PCB pcb ready list dequeue swap process pcb Now pcb contains the state the process that called down and the currently running process some other process waiters enqueue pcb end public void begin value value The value was previously negative there some process waiting must wake PCB pcb waiters dequeue ready list enqueue pcb end Semaphore The implementation swap process magic This procedure probably really written assembly language but will describe Java Assume the CPU current stack pointer register accessible void swap process PCB pcb int new pcb saved pcb saved new mentioned earlier each process has its own stack with stack frame for each procedure that process has called but not yet completed Each stack frame contains the very least enough information implement return from the procedure the address the instruction that called the procedure and pointer the caller stack frame Each CPU devotes one its registers call point the current stack frame the process currently running When the CPU encounters return statement reloads its and program counter registers from the stack frame approximate description pseudo Java might something like this class StackFrame int callers int callers StackFrame the current stack pointer Here how return instruction address return point callers callers goto return point course there isn really goto statement Java and this would all done the hardware sequence assembly language statements Suppose process calls swap process pcb where pcb saved points stack frame representing call swap process some other process The call swap process creates frame stack and makes point The second statement swap process saves pointer that stack frame pcb The third statement then loads with pointer stack frame for swap process Now when the procedure returns will return whatever procedure called swap process process Implementing Critical Sections The final piece the puzzle implement begin and end There are several ways doing this depending the hardware configuration First suppose there are multiple CPU accessing single shared memory unit Generally the memory bus hardware serializes requests read and write memory words For example two CPU try write different values the same memory word the same time the net result will one the two values not some combination the values Similarly one CPU tries read memory word the same time another modifies the read will return either the old new value will not see half changed memory location Surprisingly that all the hardware support need implement critical sections The first solution this problem was discovered the Dutch mathematician Dekker simpler solution was later discovered Gary Peterson Peterson solution looks deceptively simple see how tricky the problem let look couple simpler but incorrect solutions For now will assume there are only two processes and The first idea have the processes take turns shared int turn depending void begin int process version begin while turn nothing void end int process version end turn give the other process chance This solution certainly safe that never allows both processes their critical sections the same time The problem with this solution that not live process wants enter its critical section and turn will have wait until process decides enter and then leave its critical section Since will only used critical sections protect short operations see the implementation semaphores above reasonable assume that process that has done begin will soon end but the converse not true There reason assume that the other process will want enter its critical section any time the near future even all get around this problem second attempt solve the problem uses shared array critical indicate which processes are their critical sections shared boolean critical false false void begin int critical true while critical nothing void end int critical false This solution unfortunately prone deadlock both processes set their critical flags true the same time they will each loop forever waiting for the other process ahead switch the order the statements begin the solution becomes unsafe Both processes could check each other critical states the same time see that they were false and enter their critical sections Finally change the code void begin int critical true while critical critical false perhaps sleep for while critical true livelock can occur The processes can get into loop which each process sets its own critical flag notices that the other critical flag true clears its own critical flag and repeats Peterson correct solution combines ideas from both these attempts Like the second solution each process signals its desire enter its critical section setting shared flag Like the first solution uses turn variable but only uses break ties shared int turn shared boolean critical false false void begin int critical true let other guy know trying turn nice let him first while critical the other guy trying turn and has precedence nothing void end int critical false done now Peterson solution while correct has some drawbacks First employs busy wait sometimes called spin lock which bad for reasons suggested above However critical sections are only used protect very short sections code such the down and operations semaphores above this isn too bad problem Two processes will only rarely attempt enter their critical sections the same time and even then the loser will only have spin for brief time more serious problem that Peterson solution only works for two processes Next present three solutions that work for arbitrary numbers processes Most computers have additional hardware features that make the critical section easier solve One such feature test and set instruction that sets memory location given value and the same time records the CPU unshared state information about the location previous value For example the old value might loaded into register condition code might set indicate whether the old value was zero Tanenbaum presents Fig page solution using test and set Here version using Java like syntax shared boolean lock false true any process its void begin same for all processes for boolean key testAndSet lock key return void end lock false Some other computers have swap instruction that swaps the value register with the contents shared memory word shared boolean lock false true any process its void begin same for all processes boolean key true for swap key lock key return void end boolean key false swap key lock The problem with both these solutions that they not necessarily prevent starvation several processes try enter their critical sections the same time only one will succeed safety and the winner will chosen bounded amount time liveness but the winner chosen essentially randomly and there nothing prevent one process from winning all the time The bakery algorithm Leslie Lamport solves this problem When process wants get service takes ticket The process with the lowest numbered ticket served first The process are used break ties static final int number processes shared boolean choosing false false false shared int ticket void begin int choosing true ticket max ticket ticket choosing false for int while choosing nothing while ticket ticket ticket ticket ticket nothing void end int ticket Finally note that all these solutions the critical section problem assume multiple CPU sharing one memory there only one CPU cannot afford busy wait However the good news that don have All have make sure that the short term scheduler discussed the next section does not switch processes while process critical section One way this simply block interrupts Most computers have way preventing interrupts from occurring can dangerous block interrupts for extended period time but fine for very short critical sections such the ones used implement semaphores Note that process that blocks semaphore does not need mutual exclusion the whole time blocked the critical section only long enough decide whether block Short term Scheduling Earlier called process that not blocked runnable and said that runnable process either ready running general there list runnable processes called the ready list Each CPU picks process from the ready list and runs until blocks then chooses another process run and The implementation semaphores above illustrates this This switching among runnable processes called short term scheduling and the algorithm that decides which process run and how long run called short term scheduling policy discipline Some policies are preemptive meaning that the CPU may switch processes even when the current process isn blocked Before look various scheduling policies worthwhile think about what are trying accomplish There tension between maximizing overall efficiency and giving good service individual customers From the system point view two important measures are Throughput The amount useful work accomplished per unit time This depends course what constitutes useful work One common measure throughput jobs minute second hour depending the kinds job Utilization For each device the utilization device the fraction time the device busy good scheduling algorithm keeps all the devices CPU disk drives etc busy most the time Both these measures depend not only the scheduling algorithm but also the offered load load very light jobs arrive only infrequently both throughput and utilization will low However with good scheduling algorithm throughput should increase linearly with load until the available hardware saturated and throughput levels off Each job also wants good service general good service means good response starts quickly runs quickly and finishes quickly There are several ways measuring response Turnaround The length time between when the job arrives the system and when finally finishes Response Time The length time between when the job arrives the system and when starts produce output For interactive jobs response time might more important than turnaround Waiting Time The amount time the job ready runnable but not running This better measure scheduling quality than turnaround since the scheduler has control the amount time the process spends computing blocked waiting for Penalty Ratio Elapsed time divided the sum the CPU and demands the the job This still better measure how well the scheduler doing measures how many times worse the turnaround than would ideal system the job never had wait for another job could allocate each device soon wants and experienced overhead for other operating system functions would have penalty ratio takes twice long complete would the perfect system has penalty ration measure the overall performance can then combine the performance all jobs using any one these measures and any way combining For example can compute average waiting time the average waiting times all jobs Similarly could calculate the sum the waiting times the average penalty ratio the variance response time etc There some evidence that high variance response time can more annoying interactive users than high mean within reason Since are concentrating short term CPU scheduling one useful way look process sequence bursts Each burst the computation done process between the time becomes ready and the next time blocks the short term scheduler each burst looks like tiny job First Come First Served The simplest possible scheduling discipline called First come first served FCFS The ready list simple queue first first out The scheduler simply runs the first job the queue until blocks then runs the new first job and When job becomes ready simply added the end the queue Here example which will use illustrate all the scheduling disciplines Burst Arrival Time Burst Length All times are milliseconds The following Gantt chart shows the schedule that results from FCFS scheduling The main advantage FCFS that easy write and understand but has some severe problems one process gets into infinite loop will run forever and shut out all the others Even assume that processes don have infinite loops take special precautions catch such processes FCFS tends excessively favor long bursts Let compute the waiting time and penalty ratios for these jobs Burst Start Time Finish Time Waiting Time Penalty Ratio Average you can see the shorted burst has the worst penalty ratio The situation can much worse short burst arrives after very long one For example suppose burst length arrives time and burst length arrives immediately after time The first burst doesn have wait all its penalty ratio perfect but the second burst waits milliseconds for penalty ratio Favoring long bursts means favoring CPU bound processes which have very long CPU bursts between operations general would like favor bound processes since give the CPU bound process will quickly finish its burst start doing some and get out the ready list Consider what happens have one CPU bound process and several bound processes Suppose start out the right foot and run the bound processes first They will all quickly finish their bursts and start their operations leaving run the CPU bound job After while they will finish their and queue behind the CPU bound job leaving all the devices idle When the CPU bound job finishes its burst will start operation allowing run the other jobs before they will quickly finish their bursts and start Now have the CPU sitting idle while all the processes are doing Since the CPU hog started its first will likely finish first grabbing the CPU and making all the other processes wait The system will continue this way alternating between periods when the CPU busy and all the devices are idle with periods when the CPU idle and all the processes are doing have destroyed one the main motivations for having processes the first place allow overlap between computation with This phenomenon called the convoy effect summary although FCFS simple performs poorly terms global performance measures such CPU utilization and throughput also gives lousy response interactive jobs which tend bound The one good thing about FCFS that there starvation Every burst does get served waits long enough Shortest Job First much better policy called shortest job first SJF Whenever the CPU has choose burst run chooses the shortest one The algorithm really should called shortest burst first but the name SJF traditional This policy certainly gets around all the problems with FCFS mentioned above fact can prove the SJF optimal with respect average waiting time That any other policy whatsoever will have worse average waiting time decreasing average waiting time also improve processor utilization and throughput Here the proof that SJF optimal Suppose have set bursts ready run and run them some order other than SJF Then there must some burst that run before shorter burst say run before but reversed the order would increase the waiting time but decrease the waiting time Since have net decrease total and hence average waiting time Continuing this manner move shorter bursts ahead longer ones eventually end with the bursts sorted increasing order size think this bubble sort Here our previous example with SJF scheduling Burst Start Time Finish Time Waiting Time Penalty Ratio Average Here the Gantt chart described SJF non preemptive policy There also preemptive version the SJF which sometimes called shortest remaining time first SRTF Whenever new job enters the ready queue the algorithm reconsiders which job run the new arrival has burst shorter than the remaining portion the current burst the scheduler moves the current job back the ready queue the appropriate position considering the remaining time its burst and runs the new arrival instead With SJF SRTF starvation possible very long burst may never get run because shorter bursts keep arriving the ready queue will return this problem later There only one problem with SJF SRTF don know how long burst going until run Luckily can make pretty good guess Processes tend creatures habit one burst process long there good chance the next burst will long well Thus might guess that each burst will the same length the previous burst the same process However that strategy won work well process has occasional oddball burst that unusually long short burst Not only will get that burst wrong will guess wrong the next burst which more typical for the process better idea make each guess the average the length the immediately preceding burst and the guess used before that burst guess guess previous burst This strategy takes into account the entire past history process guessing the next burst length but quickly adapts changes the behavior the process since the weight each burst computing the guess drops off exponentially with the time since that burst call the most recent burst length the one before that etc then the next guess Round Robin and Processor Sharing Another scheme for preventing long bursts from getting too much priority preemptive strategy called round robin keeps all the burst queue and runs the first one like FCFS But after length time called quantum the current burst hasn completed moved the tail the queue and the next burst started Here are Gantt charts our example with round robin and quantum sizes and With get average waiting time and average penalty ratio work out yourself With the averages drop and respectively The limit approaches zero called processor sharing causes the CPU shared equally among all the ready processes the steady state when bursts enter leave the ready list each burst sees penalty ratio exactly the length the ready queue course only theoretical interest There substantial overhead switching from one process another the quantum too small the CPU will spend most its time switching between processes and practically none actually running them Priority Scheduling There are whole family scheduling algorithms that use priorities The basic idea always run the highest priority burst Priority algorithms can preemptive non preemptive burst arrives that has higher priority than the currently running burst does switch immediately wait until the current burst finishes Priorities can assigned externally processes based their importance They can also assigned and changed dynamically For example priorities can used prevent starvation raise the priority burst the longer has been the ready queue eventually will have the highest priority all ready burst and guaranteed chance finish One interesting use priority sometimes called multi level feedback queues MLFQ maintain sequence FIFO queues numbered starting zero New bursts are added the tail queue always run the burst the head the lowest numbered non empty queue doesn complete complete within specified time limit moved the tail the next higher queue Each queue has its own time limit one unit queue two units queue four units queue eight units queue etc This scheme combines many the best features the other algorithms favors short bursts since they will completed while they are still low numbered high priority queues Long bursts the other hand will run with comparatively few expensive process switches This idea can generalized Each queue can have its own scheduling discipline and you can use any criterion you like move bursts from queue queue There end the number algorithms you can dream Analysis possible analyze some these algorithms mathematically There whole branch computer science called queuing theory concerned with this sort analysis Usually the analysis uses statistical assumptions For example common assume that the arrival new bursts Poisson The expected time wait until the next new burst arrives independent how long has been since the last burst arrived other words the amount time that has passed since the last arrival clue how long will until the next arrival You can show that this case the probability arrival the next milliseconds where parameter called the arrival rate The average time between arrivals Another common assumption that the burst lengths follow similar exponential distribution the probability that the length burst less than where another parameter the service rate The average burst length This kind system called queue The ratio particular interest burst are arriving the average faster than they are finishing the ready queue grows without bound course that can happen because there most one burst per process but this theory arrivals and departures are perfectly balanced can shown that for FCFS the average penalty ratio for bursts length you can see decreases the penalty ratio increases proving that FCFS doesn like short bursts Also note that approaches one the penalty ration approaches infinity For processor sharing noticed above all processes have penalty ratio that the length the queue can shown that the average that length will see medium term and long term scheduling later the course job might batch job such printing run paychecks interactive login session command issued interactive session might consist single process group related processes Actually and are supposed the Greek letters alpha beta and rho but can figure out how make them HTML solomon wisc edu Thu Oct CST Copyright Marvin Solomon All rights reserved 