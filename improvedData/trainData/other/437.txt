MIME Version Server CERN Date Monday Dec GMT Content Type text html Content Length Last Modified Thursday Oct GMT AntiAliasing videos and images using stochastic sampling Next Introduction AntiAliasing videos and images using stochastic sampling Arun Verma Introduction Problem description Literature review Design the Anti Aliasing module Problem Setting Choice Methods Images and Animations used Description the Algorithm Math involved Platform Requirements Results Installation Conclusions References About this document MIME Version Server CERN Date Monday Jan GMT Content Type text html Content Length Last Modified Friday Sep GMT Institutes Computer Giants Motorola Microsoft Research and Technical Reports MCC NASA Nations Instruments Apple IBM Intel Sun DEC Oracle Electrotechnical Laboratory Sony Computer Science Inc NEXOR advanced company CMU Stanford Berkeley MIT UIUC UCLA Oxford Computer Science Departments Across the Web Electrical Engineering Academic Programs This page last modified May For comments you are welcome send email hqliu utexas edu Date Wed Jan GMT Server NCSA Last modified Mon Oct GMT Content type text html Content length Andersen Fall ScheduleDr Sandy Andersen Fall ScheduleMondayTuesdayWednesdayThursdayFriday IACC IACC IACC Office Hour Office Hour Office Hour LUNCH IACC IACC IACC XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX also appointment mail andersen badlands NoDak edu phone Date Mon Nov GMT Server NCSA Content type text html Last modified Tue Apr GMT Content length What Importance Based Feature Extraction OutlineThe main ideaAn example from real lifeA neural net example with Gaussian detector nodesWhat new about importance based feature extraction The main idea Suppose that autonomous agent has feature detectors which identify its state and that uses reinforcement learning learn succeed its environment Importance based feature extraction aims tune the agent feature detectors most sensitive states where the agent choice action critical the agent action from this state will have little bearing the agent future success say that the state unimportant define important state one having very different predictions reinforcement for taking different actions terms learning this means that important state one from which the different actions have very different values Important states are not necessarily the most frequently seen Frequency based feature extraction can misled frequently occurring red herring states and may miss states which represent rare opportunities For example the agent frequently finds itself particular state where almost any action equally good frequency based feature tuning would cluster detectors around that state however those detectors will little use the agent because its choice action this state has little bearing its future reinforcement the agent may find itself rarely seen state where its choice action critical for future success such state important though infrequent course this view based the assumption that the agent task act way which optimizes its reinforcement regardless its understanding aspects the world which have bearing its strategy selection Furthermore important states need not associated with the most extreme reinforcement values For example there may state from which the agent will fail matter what action takes Therefore this state will strongly associated with failure and most likely extremely negative reinforcement values But detecting this state not very helpful the agent because when this state there nothing can prevent failure The agent would better off using detectors identify the state from which made some critical mistake That would state from which correct action might have led success instead failure The values this state might not great absolute magnitude those associated with state from which the agent always fails state from which the agent always succeeds But since some actions from this state lead success and some failure has greater span values associated with the actions this makes important state example from real life Cognitive economy one the principles use cope with daily life People tend use broad classifications whenever possible because this allows them apply information learned from few samples any large collection objects they avoid having learn how handle each new individual object But this kind stereotyping only use the classifications relate our goals and the feedback receive When our goals require respond some particular features individual need learn recognize those features which make this individual special case For example consider the concept snow concept snow has with whether can pack into snowball wet snow not fluffy snow Otherwise snow just something pretty that piles and requires get out the shovel But skiers talk about more varieties snow and the distinctions are relevant them because different kinds snow will have different effects their skiing may not remember all the varieties snow which skier friends have spoken this not necessarily comment memory but more likely due the fact that not ski and derive benefit from knowing the distinctions Supposedly Eskimos have words for many different kinds snow But someone who lived their whole life close the equator snow might simply snow some form white precipitation which they never seen each case are alloting cognitive resources for those distinctions which relate our goals This example importance based feature extraction since are tuning our feature detectors respond those features which make difference the things have and otherwise falling back broad stereotypes neural net example with Gaussian detector nodes common architecture for reinforcement learning agent feed forward connectionist network which has inputs layer hidden nodes and output layer nodes which control action selection can think the hidden nodes feature detectors which provide distributed representation the current system state Importance based feature extraction attempts tune the feature detectors according their importance selecting the agent actions detector considered important the links from the outputs have very different weights the weights were all the same that detector would contribute the same impulse each the competing output nodes and thus would have influence the agent choice action Since the link weights are used calculate values detector with sizable spread weights its outgoing links represents state from which different actions have very different expectations reinforcement other words this detector valuable because detects state from which the agent choice action will strongly affect the agent liklihood success system having Gaussian detector nodes importance based feature extraction tunes their centers order maximize each detector estimate its importance have found convenient define the importance detector the variance the weights its links the output nodes however alternative definitions are certainly possible What new about importance based feature extraction reinforcement learning problems the sparseness the feedback increases the difficulty feature extraction Importance based feature extraction addresses this problem relying bottom feature extraction Other examples this general approach include the use bottom clustering methods such Kohonen Self Organized Map Chapman Kaelbling use relevance criterion and statistical approaches built around principal component analysis Bottom clustering methods are based the frequency states Kohonen Self Organized Map and related clustering methods attempt distribute the feature detectors according the probability density function the states seen the agent contrast importance based feature extraction recognizes that autonomous agent the important states are not necessarily the most frequent noted above What the agent needs not detect commonly seen states but important states states which matter terms the action decisions the agent must make The Self Organized Map was designed for different type problem that modelling some feature domain and producing brain like mapping from inputs common features Here there reinforcement and the topological structure the feature space what important But control task the frequency based approach blind toward the reinforcement and the reinforcement what makes some states more important than others the agent Chapman Kaelbling concept relevance biases feature extraction toward the detection features which are associated with extreme reinforcement values discussed above extreme reinforcement values not necessarily indicate important state from which the agent choice action really matters Relevance tuning produces feature detectors which are relevant predicting the agent future success but which may not relevant choosing its next action When the agent detects feature all its actions will produce outcomes which are equally good that feature doesn make any difference determining its strategy even the feature relevant predicting its future success Relevance tuning cannot tell that such features are unimportant Rarely are developments neural networks unanticipated the field statistics although researchers may not recognize the common threads first glance But not aware concept like importance based feature extraction statistics Principal component analysis can very efficiently give the structure the feature space but blind toward the reinforcement seen the agent Therefore like the other approaches cannot guide feature extraction according the reinforcements the agent receives for various state action combinations under its current performance task Return Top this documentDJF Reinforcement Learning Pagefinton wisc edu November 