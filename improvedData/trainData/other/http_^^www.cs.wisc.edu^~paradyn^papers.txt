Date Tue Nov GMT Server NCSA Content type text html Last modified Thu Sep GMT Content length Paradyn Project PapersParadyn Project Papers The papers listed this page have been produced the Paradyn Project You can retrieve PostScript copy any the papers listed clicking the paper title For this work automatically you must have PostScript previewer your system few cases the document HTML The Paradyn Parallel Performance Measurement Tools Barton Miller Mark Callaghan Jonathan Cargille Jeffrey Hollingsworth Bruce Irvin Karen Karavanic Krishna Kunchithapadam and Tia Newhall IEEE Computer November Special issue performance evaluation tools for parallel and distributed computer systems Note this paper contains several color postscript pages should print acceptably printers Paradyn performance measurement tool for parallel and distributed programs Paradyn uses several novel technologies that scales long running programs hours days and large thousand node systems and automates much the search for performance bottlenecks can provide precise performance data down the procedure and statement level Paradyn based dynamic notion performance instrumentation and measurement Unmodified executable files are placed into execution and then performance instrumentation inserted into the application program and modified during execution The instrumentation controlled the Performance Consultant module that automatically directs the placement instrumentation The Performance Consultant has well defined notion performance bottlenecks and program structure that can associate bottlenecks with specific causes and specific parts program Paradyn controls its instrumentation overhead monitoring the cost its data collection limiting its instrumentation user controllable threshold The instrumentation Paradyn can easily configured accept new operating system hardware and application specific performance data also provides open interface for performance visualization and simple programming library allow these visualizations interface Paradyn Paradyn can gather and present performance data terms high level parallel languages such data parallel Fortran and can measure programs massively parallel computers workstation clusters and heterogeneous combinations these systems Dynamic Instrumentation API proposed Jeffrey Hollingsworth Barton Miller Adaptive Cost Model for Parallel Program Instrumentation Jeffrey Hollingsworth and Barton Miller Europar Lyon France August Software based instrumentation frequently used measure the performance parallel and distributed programs However using software instrumentation can introduce serious perturbation the program being measured this paper present new data collection cost system that provides programmers with feedback about the impact data collection having their application addition introduce technique that permits programmers define the perturbation their application can tolerate and then are able regulate the amount instrumentation ensure that threshold not exceeded also describe implementation the cost model and presents results from using measure the instrumentation overhead for several real applications Dynamic Program Instrumentation for Scalable Performance Tools Jeffrey Hollingsworth Barton Miller and Jon Cargille SHPCC Knoxville May this paper present new technique called dynamic instrumentation that provides efficient scaleable yet detailed data collection for large scale parallel applications Our approach unique because defers inserting any instrumentation until the application execution can insert change instrumentation any time during execution Instrumentation inserted modifying the application binary image This permits insert only the instrumentation that necessary for the current analysis being performed visualization presented result our technique collects several orders magnitude less data than traditional data collection approaches have implemented prototype our dynamic instrumentation the Thinking Machines and present results for several real applications addition include recommendations operating system designers compiler writers and computer architects about the features necessary permit efficient monitoring large scale parallel systems Dynamic Control Performance Monitoring Large Scale Parallel Systems Jeffrey Hollingsworth and Barton Miller International Conference Supercomputing Tokyo July Performance monitoring large scale parallel computers creates dilemma need collect detailed information find performance bottlenecks yet collecting all this data can introduce serious data collection bottlenecks the same time users are being inundated with volumes complex graphs and tables that require performance expert interpret present new approach called the cubed Search Model that addresses both these problems combining dynamic the fly selection what performance data collect with decision support assist users with the selection and presentation performance data Our goal provide users with answers their performance questions and the same time dramatically reduce the volume performance data need collect present case study describing how prototype implementation our technique was able identify the bottlenecks three real programs addition were able reduce the amount performance data collected factor ranging from compared traditional sampling and trace based instrumentation techniques Finding Bottlenecks Large scale Parallel Programs Jeffrey Hollingsworth Thesis August Note this paper contains several color postscript pages This thesis addresses the problem trying locate the source performance bottlenecks large scale parallel and distributed applications Performance monitoring creates dilemma identifying bottleneck necessitates collecting detailed information yet collecting all this data can introduce serious data collection bottlenecks the same time users are being inundated with volumes complex graphs and tables that require performance expert interpret have developed new approach that addresses both these problems combining dynamic the fly selection what performance data collect with decision support assist users with the selection and presentation performance data The approach called the Search Model pronounced cubed make possible implement the Search Model have developed new monitoring technique for parallel programs called Dynamic Instrumentation The premise work that not only possible line performance debugging but for large scale parallelism mandatory The Search Model closes the loop between data collection and analysis Searching for performance problem iterative process refining the answers three questions why the application performing poorly where the bottleneck and when does the problem occur answer the why question tests are conducted identify the type bottleneck synchronization computation Answering the where question isolates performance bottleneck specific resource used the program disk system synchronization variable procedure Answering when problem occurs tries isolate bottleneck specific phase the program execution Dynamic Instrumentation differs from traditional data collection because defers selecting what data collect until the program running This permits insertion and alteration the instrumentation during program execution also features new type data collection that combines the low data volume sampling with the accuracy tracing Instrumentation precisely count and time events inserted dynamically modifying the binary program These counters and timers are then periodically sampled provide intermediate values the Search Model Based this intermediate data changes are made the instrumentation collect more information further isolate the bottleneck have built prototype implementation the Search Model and Dynamic Instrumentation The prototype runs Thinking Machine and network workstations running PVM one study the tools identified the bottlenecks several real programs using two three orders magnitude less data than traditional techniques another study Dynamic Instrumentation was used monitor long running programs and introduced less than perturbation While the Search Model and Dynamic Instrumentation complement each other they are also useful individually The Search Model can applied existing post mortem performance tools even simulated machines and environments Dynamic Instrumentation has used collect performance data for other uses including visualization The Search Model and Dynamic Instrumentation are being incorporated into the Paradyn Performance Tools Mapping Performance Data for High Level and Data Views Parallel Program Performance Bruce Irvin and Barton Miller International Conference Supercomputing Philadelphia May Note this paper contains several color postscript pages should print acceptably printers Programs written high level parallel languages need profiling tools that provide performance data terms the semantics the high level language But high level performance data can incomplete when the cause performance problem cannot explained terms the semantics the language also need the ability view the performance the underlying mechanisms used the language and correlate the underlying activity the language source code The key techniques for providing these performance views the ability map low level performance data the language abstractions identify the various kinds mapping information that needs gathered support multiple views performance data and describe how can mine mapping information from the compiler and run time environment also describe how use this information produce performance data the higher levels and how present this data terms both the code and parallel data structures have developed implementation these mapping techniques for the data parallel Fortran language running the TMC have augmented the Paradyn Parallel Performance Tools with these mapping and high level language facilities and used them study several real data parallel Fortran Fortran applications Our mapping and high level language techniques allowed quickly understand these applications and modify them obtain significant performance improvements Mechanisms for Mapping High Level Parallel Performance Data Bruce Irvin and Barton Miller ICPP Workshop Challenges for Parallel Processing Chicago August Note this paper contains several color postscript pages should print acceptably printers primary problem the performance measurement high level parallel programming languages map low level events high level programming constructs discuss several aspects this problem and presents three methods with which performance tools can map performance data and provide accurate performance information programmers particular discuss static mapping dynamic mapping and new technique that uses data structure called the set active sentences Because each these methods requires coopera tion between compilers and performance tools describe the nature and amount cooperation required The three mapping methods are orthogonal describe how they should combined complete tool Although concentrate mapping upward through layers abstraction our techniques are independent mapping direction Performance Tool for High Level Parallel Programming Languages Bruce Irvin and Barton Miller IFIP Working Conference Programming Environments for Massively Parallel Distributed Systems Ascona Switzerland April Users high level parallel programming languages require accurate performance information that relevant their source code Furthermore when their programs cause performance problems the lowest levels their hardware and software systems programmers need able peel back layers abstraction examine low level problems while maintaining references the high level source code that ultimately caused the problem this paper present model for the explanation performance information for programs built multiple levels abstraction level abstraction includes collection nouns code and data objects verbs activities and performance information measured for the nouns and verbs Performance information mapped from level level maintain the relationships between low level activities and high level code even when such relationships are implicit have used the model build ParaMap performance tool for the Fortran language that has practice guided substantial improvements real Fortran applications describe the design and implementation our tool and show how its simple tabular and graphical performance displays helped find performance problems two applications each case found that performance information all levels was most useful when related parallel Fortran arrays and that could subsequently reduce each application execution time more than half Integrated Visualization Parallel Program Performance Data Karen Karavanic Jussi Myllymaki Miron Livny and Barton Miller appear Environments and Tools for Parallel Scientific Computing SIAM Press Dongarra and Tourancheau eds Performance tuning parallel application involves integrating performance data from many components the system including the message passing library performance monitoring tool resource manager operating system and the application itself The current practice visualizing these data streams using separate customized tool for each source inconvenient from usability perspective and there easy way visualize the data integrated fashion demonstrate solution this problem using Devise generic visualization tool which designed allow arbitrary number different but related data streams integrated and explored visually flexible manner display data emanating from variety sources side side three case studies First interface the Paradyn Parallel Performance Tool and Devise using two simple data export modules and Paradyn simple visualization interface show several Devise Paradyn visualizations which are useful for performance tuning parallel codes and which incorporate data from Unix utilities and application output Next describe the visualization trace data from parallel application running Condor cluster workstations Finally demonstrate the utility Devise visualizations study Condor cluster activity The Paradyn Parallel Performance Tools and PVM Barton Miller Jeffrey Hollingsworth and Mark Callaghan Environments and Tools for Parallel Scientific Computing SIAM Press Dongarra and Tourancheau eds Paradyn performance tool for large scale parallel applications using dynamic instrumentation and automating the search for bottlenecks can measure long running applications production sized data sets Paradyn has recently been ported measure native PVM applications Programmers run their unmodified PVM application programs with Paradyn Paradyn automatically inserts and modifies instrumentation during the execution the application systematically searching for the causes performance problems most cases Paradyn can isolate major causes performance problems and the part the program that responsible the problem Paradyn currently runs the Thinking Machine Sun workstations and PVM currently only Suns can measure heterogeneous programs across any these platforms This paper presents overview Paradyn describes the new facility PVM that supports Paradyn and reports experience with PVM applications Optimizing Array Distributions Data Parallel Programs Krishna Kunchithapadam and Barton Miller Languages and Compilers for Parallel Computing August Data parallel programs are sensitive the distribution data across processor nodes formulate the reduction inter node communication optimization colored graph present technique that records the run time inter node communication caused the movement array data between nodes during execution and builds the colored graph and provide simple algorithm that optimizes the coloring this graph describe new data distributions that would result less inter node communication From the distribution information write compiler pragmas used the application program Using these techniques traced the execution real data parallel application written Fortran and collected the array access information computed new distributions that should provide overall reduction program execution time However compiler optimizations and poor interfaces between the compiler and runtime systems counteracted any potential benefit from the new data layouts this context provide set recommendations for compiler writers that think are needed both write efficient programs and build the next generation tools for parallel systems The techniques that have developed form the basis for future work monitoring array access patterns and generate the fly redistributions arrays Performance Measurement Tools for High Level Parallel Programming Languages Bruce Irvin Thesis October Note this paper contains several color postscript pages Users high level parallel programming languages require accurate performance information that relevant their source code Furthermore when their programs experience performance problems the lowest levels their hardware and software systems programmers need able peel back layers abstraction examine low level problems while maintaining references the high level source code that ultimately caused the problem This dissertation addresses the problems associated with providing useful performance data users high level parallel programming languages particular describes techniques for providing source level performance data programmers for mapping performance data among multiple layers abstraction and for providing data oriented views performance present model for the explanation performance information for high level parallel language programs level abstraction includes collection nouns code and data objects verbs activities and performance information measured for the nouns and verbs Performance information mapped from level level maintain relationships between low level activities and high level code even when such relationships are implicit The model has helped implement support for performance measurement high level parallel language applications two performance measurement tools ParaMap and Paradyn describe the design and implementation these tools and show how they provide performance information for Fortran programmers Finally present results measurement studies which have used ParaMap and Paradyn improve the performance variety real Fortran applications running parallel computers each case found that overall performance trends could observed the source code level and that both data views and code views performance were useful found that some performance problems could not explained the source code level these cases used the performance tools examine lower levels abstraction find performance problems found that low level information was most useful when related source level code structures and especially data structures Finally made relatively small changes the applications source code achieve substantial performance improvements Integrating Debugger and Performance Tool for Steering Krishna Kunchithapadam and Barton Miller Steering performance optimization idiom applicable many problem domains allows control and performance tuning take place during program execution Steering emphasizes the optimization and control the performance program using mechanisms that are external the program Performance measurement tools and symbolic debuggers already independently provide some the mechanisms needed implement steering tool this paper describe configuration that integrates performance tool Paradyn and debugger build steering environment The steering configuration allows fast prototyping steering policies and provides support for both interactive and automated steering Last modified Thu Sep CDT 