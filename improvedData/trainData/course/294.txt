Date Wed Nov GMT Server NCSA Last modified Thu Nov GMT Content type text html Content length CPS FALL Fall Planning Under Uncertainty Background Grading Outline Projects Jump current place outline Evolving Syllabus Background This seminar will introduce students exciting area research that draws upon results from operations research Markov decision processes machine learning reinforcement learning and traditional planning attack problems with great scientific and commercial potential will read and discuss handful recent papers that will give students appreciation for the state the art Students will undertake group research project solidify their understanding the basic concepts and perhaps contribute the field will make number presentations introduce students the background material necessary read the research papers only undergraduate level computer science knowledge and basic probability theory and calculus will assumed think there are useful contributions made researchers algorithms complexity numerical methods and systems and think people all these areas would find some useful information the seminar Everyone welcome Instructor Michael LittmanOffice LSRC Phone Email mlittman duke eduOffice hours TBA Description Research planning making sequence choices achieve some goal has been mainstay artificial intelligence for many years Traditionally the decision making models that have been studied admit uncertainty whatsoever every aspect the world that relevant the generation and execution plan known advance contrast work operations research has focussed the uncertainty actions but uses impoverished representation for specifying planning problems The purpose this seminar explore some middle ground between these two well studied extremes with the hope understanding how might create systems that can reason efficiently about plans complex uncertain worlds will review the foundational results from and and read series papers written over the last few years that have begun bridge the gap Philosophy There are basically two three papers different approaches the same basic problem that like people read and understand These papers are quite recent and represent active areas research that have been maturing quite quickly over the last few years result get deep appreciation for this work will need read number papers that introduce the necessary background approach organizing the seminar will try keep the assigned reading minimum and ask students concentrate understanding the state the field and identifying the important open research questions Prerequisites The seminar should accessible any advanced computer science student goal introduce critical background material the need arises Nonetheless need some common ground begin will assume that students are familiar with programming any language algorithm analysis big notation calculus derivates multivariate functions and probability theory conditional probabilities Postrequisites addition exploring the question how create plans that are effective uncertain environments there are number other important topics that students will learn about this seminar students will exposed Markov decision processes dynamic programming linear programming temporal difference learning supervised function approximation gradient descent and neural networks STRIPS rules and partial order planning Grading The grading policy designed stimulate students think about some the important issues this area Class grade will based class participation short homework assignments and final project OutlineOrganization Meeting Thursday September get together discuss the best time schedule the class you can make the meeting please send email mlittman duke edu Introduction What planning What uncertainty What are some applications planning under uncertainty lay out the space issues and describe the part the space explore Michael Lederman Littman Algorithms for Sequential Decision Making dissertation and Technical Report Brown University Department Computer Science Providence March Chapter Introduction local postscript local bibliography postscript Markov Decision Processes introduce the MDP model which formal specification for the particular problem will examining describe the fundamental concepts states actions transitions rewards discounting horizons results existence and dominance optimal value function optimal greedy policies and the algorithms value iteration policy iteration modified policy iteration linear programming sense these algorithms completely solve the problem planning under uncertainty The rest the seminar concerned with solving MDPs more efficiently exploiting additional structure present some instances Michael Littman Thomas Dean and Leslie Pack Kaelbling the complexity solving Markov decision problems Proceedings the Eleventh Annual Conference Uncertainty Artificial Intelligence UAI Montreal Quebec Canada postscript Homework Represent complex domain MDP Slides postscript Accelerating Solutions MDPs One class algorithms for solving MDPs more quickly restricts value iteration updates states that are likely benefit from additional computational resources Prioritized Sweeping uses heuristic for measuring when updating the value state likely important for computing approximately optimal solution quickly Andrew Moore and Christopher Atkeson Prioritized sweeping Reinforcement learning with less data and less real time Machine Learning compressed postscript Real time dynamic programming attempts find good approximate policy quickly focussing updates states that are likely visited Andrew Barto Bradtke and Satinder Singh Learning act using real time dynamic programming Artificial Intelligence compressed postscript Another approach explicitly produce good partial policy identifying states that are likely visited and solving smaller MDP Jonathan Tash and Stuart Russell Control strategies for stochastic planner Proceedings the National Conference Artificial Intelligence postscript Slides postscript Value Function Approximation The above approaches represent states being completely unrelated objects many domains states can described such way that similar states from policy value standpoint have similar representations For example any attempt create transition function for the game backgammon would likely make use board based representation states This insight can exploited exchanging the classical table based method for representating value functions one that uses function approximator for example neural net map state description vectors values wildly successful example this Gammon this work makes use several important background ideas including gradient descent and temporal difference learning that will need look well Richard Sutton Learning predict the method temporal differences Machine Learning postscript Gerald Tesauro Temporal difference learning and Gammon Communications the ACM html Slides postscript Another interesting application lambda and neural networks applied MDP like problems Crites and Barto elevator controller Robert Crites and Andrew Barto Improving elevator performance using reinforcement learning Touretzky Mozer and Hasselmo editors Advances Neural Information Processing Systems The MIT Press compressed postscript even simpler and similarly successful example for cellular phone channel assignments based and linear function approximator Satinder Singh and Dimitri Bertsekas Reinforcement learning for dynamic channel allocation cellular telephone systems appear Advances Neural Information Processing Systems The MIT Press postscript Tesauro work difficult generalize because simultaneously addresses many unsolved problems More recent work has begun tease apart the effect using function approximation dynamic programming from the use the temporal difference algorithm Justin Boyan and Andrew Moore Generalization reinforcement learning Safely approximating the value function Tesauro Touretzky and Leen editors Advances Neural Information Processing Systems The MIT Press compressed postscript Recent workshop value function approximation These results were not convincing everyone Richard Sutton Generalization reinforcement learning Successful examples using sparse coarse coding Advances Neural Information Processing Systems The MIT Press postscript Slides postscript Some the most interesting recent work has concerned theoretical results when function approximation will and will not result convergent algorithm Results exist concerning gradient descent methods and averaging methods Leemon Baird Residual algorithms Reinforcement learning with function approximation Armand Prieditis and Stuart Russell editors Proceedings the Twelfth International Conference Machine Learning Morgan Kaufmann compressed postscript html Geoffrey Gordon Stable function approximation dynamic programming Armand Prieditis and Stuart Russell editors Proceedings the Twelfth International Conference Machine Learning Morgan Kaufmann compressed postscript John Tsitsiklis and Benjamin Van Roy Feature based methods for large scale dynamic programming Machine Learning local postscript Homework Propose research project Stochastic Planning The work function approximation attempts exploit structure the state space but treats actions black box tranformations from states distributions over states promising alternative use symbolic descriptions the actions reason about entire classes state state transitions all once This the approach taken planning David McAllester and David Rosenblitt Systematic nonlinear planning Proceedings the National Conference Artificial Intelligence postscript LISP code the last two years planning algorithms have been proposed that differ substantially from the classic planners Although unconventional these planners have been shown empirically result much shorter running times several orders magnitude faster Blum and Furst algorithm views planning type graph search while Kautz and Selman reduce planning boolean satisfiability problem Avrim Blum and Merrick Furst Fast planning through planning graph analysis Proceedings the International Joint Conference Artificial Intelligence IJCAI pages extended version compressed postscript Henry Kautz and Bart Selman Pushing the envelope Planning propositional logic and stochastic search Proceedings the National Conference Artificial Intelligence postscript Slides postscript spite recent algorithmic advances the traditional view planning ignores uncertainty Uncertainty can introduced gently assuming deterministic domain with some randomness added Jim Blythe Planning with external events Proceedings the Tenth Conference Uncertainty Artificial Intelligence postscript The Buridan system introduces more general representation for stochastic STRIPS operators and extends partial order planning stochastic domains Its representation equivalent expressiveness MDPs Slides postscript Nicholas Kushmerick Steve Hanks and Daniel Weld algorithm for probabilistic planning Artificial Intelligence compressed postscript The Buridan system has been expanded that the plan representation more powerful though less powerful than policy like representation Denise Draper Steve Hanks and Dan Weld Probabilistic planning with information gathering and contingent execution Technical Report University Washington Department Computer Science Seattle December compressed postscript Slides postscript area intense interest but remarkably little work combining direct manipulation STRIPS type actions with dynamic programming based algorithm Several papers adopt the view that function approximation form abstraction the form which can derived automatically from propositional representation the planning problem Richard Dearden and Craig Boutilier Abstraction and approximate decision theoretic planning appear Artificial Intelligence postscript Craig Boutilier Richard Dearden and Moises Goldszmidt Exploiting structure policy construction appear Proceedings the International Joint Conference Artificial Intelligence postscript Craig Boutilier and Richard Dearden Approximating value trees structured dynamic programming appear Proceedings the Thirteenth International Conference Machine Learning postscript Slides postscript Summary Slides postscript Next deal with partially observable Markov decision processes and how solve them Advanced Topics make unexpectedly fast progress through the core topics there are number interesting issues could explore including hierarchical solutions MDPs partially observable MDPs solving games Here are some papers recently found that might useful discuss The first gives notation for representing multi player games incomplete information that might useful defining similar notation for MDPs The second describes how solve very large and even continuous state MDPs efficiently using linear programming Daphne Koller and Avi Pfeffer Representations and solutions for game theoretic problems Preliminary version appeared Proceedings the International Joint Conference Artificial Intelligence IJCAI Montreal Canada August pages postscript Michael Trick and Stanley Zin linear programming approach solving stochastic dynamic programs postscript Project Ideas The major thrust the seminar will undertake group project exploring some facet the problem planning under uncertainty Some sample project ideas are Are there methods efficiently evaluating plans complex domains MDP aggregation methods have anything offer Can hierarchical methods applied propositional state spaces Can Blum and Furst Graphplan algorithm extended stochastic domains How the known results concerning the use function approximation dynamic programming relate one another How does Boutilier and Dearden structured policy iteration algorithm perform some simple structured MDPs starting make some progress domain for later project development Stephen Majercik has written his ideas load balancing MDP which are accessible from his home page talked about using the GALA system basis for general declarative language for specifying MDPs Another options standard proposed Rich Sutton Last modified Thu Nov EST Michael Littman mlittman duke edu 