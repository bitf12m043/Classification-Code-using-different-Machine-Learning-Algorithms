Date Tue Dec GMT Server NCSA Content type text html Multi Service Search and Comparison Using the MetaCrawlerMulti Service Search and Comparison Using the MetaCrawler Erik Selberg Oren Etzioni Abstract Standard Web search services though useful are far from ideal There are over dozen different search services currently existence each with unique interface and database covering different portion the Web result users are forced repeatedly try and retry their queries across different services Furthermore the services return many responses that are irrelevant outdated unavailable forcing the user manually sift through the responses searching for useful information This paper presents the MetaCrawler fielded Web service that represents the next level the information food chain The MetaCrawler provides single central interface for Web document searching Upon receiving query the MetaCrawler posts the query multiple search services parallel collates the returned references and loads those references verify their existence and ensure that they contain relevant information The MetaCrawler sufficiently lightweight reside user machine which facilitates customization privacy sophisticated filtering references and more The MetaCrawler also serves tool for comparison diverse search services Using the MetaCrawler data present Consumer Reports evaluation six Web search services Galaxy InfoSeek Lycos Open Text WebCrawler and Yahoo addition also report the most commonly submitted queries the MetaCrawler Keywords MetaCrawler WWW World Wide Web search multi service multi threaded parallel comparison Introduction Web search services such Lycos and WebCrawler have proven both useful and popular the Web grows the number and variety search services increasing well Examples include the Yahoo net directory the Harvest home page search service the Query Image Content service the Virtual Tourist directory organized geographic regions and more Since each service provides incomplete snapshot the Web users are forced try and retry their queries across different indices until they find appropriate responses The process querying multiple services quite tedious Each service has its own idiosyncratic interface which the user forced learn Further the services return many responses that are irrelevant outdated unavailable forcing the user manually sift through the responses searching for useful information This paper presents the MetaCrawler search service that attempts address the problems outlined above The premises underlying the MetaCrawler are the following single search service sufficient Table shows that single service able return more then the references followed users Many references returned services are irrelevant and can removed the user better able express the query Table shows that the references returned can removed the user supplies more expressive query Low quality references can detected and removed fairly quickly Table shows that average about references can verified well under minutes while simple collation and ranking takes under seconds These features will used the Web population The MetaCrawler receiving over queries per week and that number growing shown Figure The MetaCrawler log facilitates objective evaluation and comparison the underlying search services Tables detail trade offs between the services For example Lycos returns over more followed references than any other service yet WebCrawler the fastest taking average seconds return answers queries The MetaCrawler logs also reveal that people search for wide variety information from Robins zyx music While the most common queries are related sex and pornography these only account for under the total queries submitted the Metacrawler shown Table Nearly half all queries submitted are unique The remainder this paper organized follows the design and implementation the MetaCrawler are described Section Experiments validate the above premises are described Section discuss related work Section and our ideas for future work and potential impact appear Section conclude with Section The MetaCrawler The MetaCrawler free search service used for locating information available the World Wide Web The MetaCrawler has interface similar WebCrawler and Open Text that allows users enter search string query and returns page with click able references hits pages available the Web However the internal architecture the MetaCrawler radically different from the other search services Standard Web searching consists three activities Indexing the web for new and updated pages process that demands substantial CPU and network resources Storage the Web pages retrieved into index which typically requires large amount disk space Retrieval pages matching user queries For most services this amounts returning ranked list page references from the stored index Standard search services create and store index the Web well retrieve information from that index Unlike these services the MetaCrawler meta service which uses internal database its own instead relies other external search services provide the information necessary fulfill user queries The insight here that separating the retrieval pages from indexing and storing them lightweight application such the MetaCrawler can access multiple databases and thus provide larger number potentially higher quality references than any search service tied single database Another advantage the MetaCrawler that does not depend upon the implementation existence any one search service Some indexing mechanism necessary for the Web Typically this done using automated robots spiders which may not necessarily the best choice However the underlying architecture the search services used the MetaCrawler unimportant long there central complete search service and several partial search services exist the MetaCrawler can provide the benefit accessing them simultaneously and collating the results The MetaCrawler prototype has been publicly accessible since July has been steadily growing popularity logging upwards queries per week and increasing The MetaCrawler currently accesses six services Galaxy InfoSeek Lycos Open Text WebCrawler and Yahoo works follows given query the MetaCrawler will submit that query every search service knows parallel These services then return list references WWW pages hits Upon receiving the hits from every service the MetaCrawler collates the results merging all hits returned Duplicate hits are listed only once but each service that returned hit acknowledged Expert user supplied sorting options are applied this time Optionally the MetaCrawler will verify the information existence loading the reference When the MetaCrawler has loaded reference then able score the page using supplementary query syntax supplied the user When the MetaCrawler has finished processing all the hits the user presented with page consisting sorted list references Each reference contains click able hypertext link the reference followed local page context available confidence score verified keywords and the actual URL the reference Each word the search query automatically boldfaced that may determine which references are followed each click able link returned the user points not the reference but script which logs that the reference was followed and then refers the user browser the correct URL Querying many services and simply collating results will return more results than any one service but the cost presenting the user with more irrelevant references The MetaCrawler designed increase both the number hits and relevance hits returned The MetaCrawler yields higher proportion relevant hits using both powerful query syntax well expert options that users can more easily instruct the MetaCrawler how determine the quality the returned references The query syntax used specifies required and non desired words well words that should appear phrase The expert options allow users rank hits physical location such the user country well logical locality such their Internet domain User Interface While giving the user Web form with added expressive power was easy presenting the user with form that would facilitate actually using the novel features the MetaCrawler proved challenge strove for balance between simple search form and expressive one keeping mind interface issues mentioned service providers our early designs focused syntax for queries with several additional options for improving the result This syntax was similar InfoSeek query syntax parentheses were used define phrases plus sign designated required word and minus designated non desired word For example search for John Cleese naturally requiring that both John and Cleese appear together the syntax required was the unwieldy John Cleese Not surprisingly discovered that while most users attempted use the syntax they often introduced subtle syntactical errors causing the resulting search produce entirely irrelevant set hits our current design have reduced the need for extra syntax and instead ask the user select the type search The three options are Search for words phrase Treat the query text single phrase and attempt match the phrase pages retrieved Four score and seven years ago Search for all words Attempt find each word the query text somewhere the retrieved pages This the equivalent logical and Search for any words Attempt find any word the query text the retrieved pages This the equivalent logical The older syntax still supported although not advertised prominently the main search page save for the minus sign which was the most widely used element the query syntax Since changed the search page this new design the number malformed requests has dropped significantly addition the query entry box maintain various expert options which can activated via menus The MetaCrawler currently uses two menus provide extra expressiveness The first describes coarse grain Locality with options for the user continent country and Internet domain well options select specific continent The second menu describes the sundry Internet domain types edu com etc These options allow users better describe what they are looking for terms where they believe the relevant information will Client Server Design Current search services amortize the cost indexing and storing pages over hundreds thousands retrievals per day order field the maximal number retrievals services devote minimal effort responding each individual query Increases server capacity are quickly gobbled increases pages indexed and queries per day result there little time for more sophisticated analysis filtering and post processing responses queries decoupling the retrieval pages from indexing and storage the MetaCrawler able spend time performing sophisticated analysis pages returned The MetaCrawler just retrieves data spending time indexing storing Thus the MetaCrawler relatively lightweight The prototype written comes only lines code including comments does not need the massive disk storage maintain index nor does need the massive CPU and network resources that other services require Consequently MetaCrawler client could reside comfortably individual user machine individualized MetaCrawler client that accesses multiple Web search services has number advantages First the user machine bears the load the post processing and analysis the returned references Given extra time post processing can quite sophisticated For example the MetaCrawler could divide references into clusters based their similarity each other could engage secondary search following references related pages determine potential interest Second the processing can customized the user taste and needs For example the user may choose filter advertisements parents may try block rated pages Third the MetaCrawler could support scheduled queries what new today about the Seattle Mariners storing the results previous queries the user machine the MetaCrawler can focus its output new updated pages Finally for pay per query services the MetaCrawler can programmed with selective query policies the cheapest service first even compute the optimal service querying sequence Organizations may choose have institutional MetaCrawler with enhanced caching capabilities the presumption that people within organization will want examine many the same pages The cache could also facilitate local annotations creating collaborative filtering and information exchange environment the sort described elsewhere Finally while our prototype MetaCrawler depends the good will the underlying services MetaCrawler client would not the future underlying service may choose block MetaCrawler requests which are easily identified However would nearly impossible distinguish queries issued MetaCrawler client versus queries made directly person Common Usage One the frequently asked questions regarding search the Internet What are people searching for Table summarizes the top ten repeated queries out total queries made from July through September Each query the top ten related sex However the combined top ten queries amount only queries out total queries Further queries were not repeated Table Top Ten Queries Issued the MetaCrawlerNo QueryTimes Issued sex erotica nude porn penthouse pornography erotic porno adult playboy Times Issued lists the number times the corresponding query was issued from July through Sept Note that while each query sexually related the combined total amounts less than the total queries processed the MetaCrawler Also the queries issued were unique Evaluation The MetaCrawler was released publicly July Averages and percentages presented this paper are based the completed queries starting July and ending September except those reference Open Text which are based completed queries starting September when Open Text was added the MetaCrawler repertoire The log results from seven days were omitted due service changing its output format causing that service return references the MetaCrawler even though the service was available The MetaCrawler currently running DEC Alpha under OSF The first hypothesis confirmed after deployed the MetaCrawler was that sending queries parallel and collating the results was useful confirm this used the metric that references followed from the page hits returned the MetaCrawler contained relevant information calculated the percentage references followed users for each the search services Table demonstrates the need for using multiple services while Lycos did return the plurality the hits that were followed with share the last month recorded slightly under the followed references came from the other five services Skeptical readers may argue that service providers could invest more resources and provide more comprehensive indices the web However recent studies indicate the rate Web expansion and change makes complete index virtually impossible Table Market Shares Followed References followed Jul Sept followed Sept Lycos WebCrawler InfoSeek Galaxy Open Text Yahoo This table shows the percentage each service has the total followed references References returned two more services are included under each service which why the columns sum over The table demonstrates that user who restricts his her queries single service will miss most the relevant references then analyzed the data determine which any the added features the MetaCrawler were helping users The metric used was the number references pruned Table shows the average number references removed for each advanced option Table Effect Features Removing Irrelevant HitsFeature Hits RemovedSyntax Dead Expert This table shows the percentage hits removed when particular feature was used Syntax refers queries that were removed due sophisticated query syntax minus for undesired words Dead refers unavailable inaccessible pages and Expert refers hits removed due restriction the references origins Using syntax for required non desired words typically reduces the number returned results Detecting dead pages allowed the removal another Finally the expert options were very successful removing unwanted references When all these features are used conjunction the returned references can removed MetaCrawler Benchmarks have shown that the MetaCrawler improves the quality results returned the user But what the performance cost Table shows the average times per query differentiating between having the MetaCrawler simply collate the results verify them well Table Average Time for MetaCrawler Return Results Wall Clock Time User Time System Time Lag Time Collated Verified All times are measured seconds Wall Clock Time the total time taken for average query and broken down into User System and Lag Time User Time the time taken the MetaCrawler program System Time the time taken the operating system and Lag Time the time taken for pages downloaded off the network Table shows that the MetaCrawler finished relatively quickly The average time return collated results little over seconds longer than the slowest service shown Table This expected given the percentage the time service times out which causes the MetaCrawler wait for full minute before returning all the results are pleased with the times reported for verification Our initial prototype typically took five minutes perform verification recently began caching retrieved pages for three hours and have found that caching reduces the average verification time nearly one half are confident that this time can further reduced more aggressive caching well improvements the thread management used the MetaCrawler Since the MetaCrawler was publicly announced the daily access count has been growing linear rate are also pleased with increased use the user options Figure plots the data points for the weeks beginning July until September Feature Use Week shows the number queries where any the MetaCrawler advanced features such verification were used Figure Queries per week from July Sept Search Service Comparison addition validating our claims the MetaCrawler logs also allow present Consumer Reports style comparison the search services evaluate each service using three metrics Coverage How many hits will returned average Relevance Are hits returned actually followed users Performance How long does each service take and how often does time out Coverage Given pre set maximum the number hits returned each service measured both the percentage references returned well references unique the service that returned them Thus returned with unique shows that average service returns its maximum allowed with those hits being unique Tables and details our findings terms raw content shows that with default parameters Open Text returns the maximum hits allowed with nearly those hits being unique Lycos and WebCrawler follow also returning over with slightly over those hits being unique Yahoo has particularly poor performance the total hits metric but this was not surprising included Yahoo the hypothesis that people search for subjects such Mariners Baseball which Yahoo excels However turned out this hypothesis was incorrect people tended use the MetaCrawler search for nuggets information such Ken Griffey hand injury Yahoo does not index this type information and thus shows poor content Presumably most topic searches Yahoo directly Although each service returns mostly unique references not clear whether those references are useful Further unique references are not necessarily unique service database another service could return that reference given different query string Table Returned References Service Max Hits Returned Ave Hits Returned Maximum Allowed Open Text Lycos WebCrawler Galaxy InfoSeek Yahoo The first column shows the percentage the maximum hits allowed that each service returned Each percentage was calculated dividing the average hits returned the maximum allowed for that service shown the second column This percentage measure how many hits service will provide given pre set maximum The MetaCrawler used different maximum values for services some had internal maximum values and others would either accept only certain maximums none all Table Unique References Service Unique Hits Returned Galaxy Yahoo Lycos WebCrawler Open Text InfoSeek This table shows the percentage references each service returned that were unique that service Relevance measure relevance two metrics are used The first which service returning the most references that people follow This shown Table The second metric what percent references returned each service are people following Table summarizes these calculations shows that nearly all references returned InfoSeek were followed Lycos Open Text and WebCrawler follow each having about their hits followed This data has two caveats The first that the relevant information for people may the list references itself For example people who wish see how many links there are their home page may search their own name just calculate this number The second caveat that these numbers may skewed the number hits returned each service Thus while InfoSeek has nearly its results followed only total references returned InfoSeek were followed compared with the references followed that were contributed Lycos which average had only its hits examined Table Relevance Returned Hits Service Hits Returned that are Followed Total Hits Followed InfoSeek Lycos Open Text WebCrawler Yahoo Galaxy This table shows the percentage followed hits for each service References returned multiple services are counted multiple times Column shows the actual number references followed for each service These numbers are out queries except Open Text which out queries Performance Finally measure each service response time Table summarizes our findings not surprising although disappointing find that average times vary from just under seconds just under The percent time the services timed out under for all services except Open Text which the newest and presumably still going through some growing pains One explanation for the length times taken these services that the majority requests are during peak hours Thus results are naturally skewed towards the times when the services are most loaded Times during non peak hours are much lower Table Performance Services Ave Time sec Timed Out WebCrawler InfoSeek Open Text Yahoo Lycos Galaxy This table shows the average time seconds taken each service fulfill query The second column gives the percent time that the service would time out fail return any hits under one minute Related Work Unifying several databases under one interface far from novel Many companies such PLS Lexis Nexis and Verity have invested several years and substantial capital creating systems that can handle and integrate heterogeneous databases Likewise with the emergence many Internet search services there have been many different efforts create single interfaces the sundry databases Perhaps the most widely distributed CUSI the Configurable Unified Search Index which large form which allows users select one service time and query that service There are also several other services much like CUSI such the All One Search Page Search Engine list Unfortunately while the user has many services these lists choose from there parallelism collation The user must submit queries each service individually although this task made easier having form interfaces the various services one page The Harvest system has many similarities the MetaCrawler however rather than using existing databases they are and post processing the information returned Harvest uses Gatherers index information and Brokers provide different interfaces extract this information However while Harvest may have many different interfaces many different internal services still search service like Lycos and WebCrawler instead meta service like MetaCrawler There are also other parallel Web search services Sun Microsystems supports very primitive service and IBM has recently announced infoMarket which rather than integrating similar services with different coverage integrates quite different services such DejaNews USENET news search service McKinley clone Yahoo with some editorial ratings various pages addition Open Text and Yahoo The closest work the MetaCrawler SavvySearch independently created multi threaded search service released May SavvySearch has larger repertoire search services although some are not WWW resource services such Roget Thesaurus SavvySearch main focus categorizing users queries and sending them the most appropriate subset its known services Like the MetaCrawler the Internet Softbot meta service that leverages existing services and collates their results The Softbot enables human user state what she wants accomplished The Softbot attempts disambiguate the request and dynamically determine how and where satisfy utilizing wide range Internet services Unlike the MetaCrawler which focuses indices and keyword queries the Softbot accesses structured services such Netfind and databases such Inspec The Softbot explicitly represents the semantics the services enabling chain together multiple services response user request The Softbot utilizes automatic planning technology dynamically generate the appropriate sequence service accesses While the MetaCrawler and the Softbot rely radically different technologies the vision driving both systems the same Both seek provide expressive and integrated interface Internet services Future Work are investigating how the MetaCrawler will scale use new services particular importance how collate results returned from different types Internet indices such USENET news and Web pages Also important determining useful methods for interacting with specialized databases such the Internet Movie Database the information requested obviously located some special purpose databases than does not make sense query each and every service are investigating methods that will enable the MetaCrawler determine which services will return relevant data based solely the query text and other data provided the user Future Design The existing MetaCrawler prototype can cause substantial network load when attempts verify large number pages While one query itself problem multiple queries occurring the same time can cause the system and network bog down However with the emergence universally portable Internet friendly languages such Java Magic Cap load problems can lessened having users machines take the workload needed perform their individual query discussed Section The JavaCrawler prototype next generation MetaCrawler written Java supports most the features already present the MetaCrawler However instead users running queries one central service each user has local copy the JavaCrawler and uses that copy directly send queries services The load caused verification will taken the user machine rather than the central server This has the added benefit inserting downloaded pages into the local cache making retrieval those pages nearly instantaneous The JavaCrawler loaded automatically from the MetaCrawler home page when visited with Java compatible browser Impact Search Service Providers anticipate that wide range meta services like the MetaCrawler will emerge over the next few years However uncertain what the relationship between these meta services and search service providers will envision that this relationship will hinge what form the information economy used service providers takes discuss two different models Charge per Access the charge per access model service providers benefit from any access their database InfoSeek has already taken this model with their commercial service InfoSeek financially rewarded regardless who what sends query their commercial database Many other databases both and off the Web also use this model The MetaCrawler fits well with this model Since service providers benefit from any access the added exposure generated the MetaCrawler their advantage Further this model creates implicit sanity check the claims this paper makes the use its features order for the MetaCrawler any meta service survive such economy must charge more per transaction than the underlying services the MetaCrawler will turn have pay each service for its information Thus users must willing pay the premium for the service voting with their pocketbook they can determine those features are truly desirable Advertising the advertising model service providers benefit from sponsors who turn gain benefit from exposure provided the service Nearly all major search services that not charge users directly have adopted this model have many other unrelated services which are heavily accessed Under this model the providers relationship with the MetaCrawler can become problematic the MetaCrawler filters away superfluous information such advertisements One promising method ensure profitable existence use provider created interfaces Providers could create interface for the MetaCrawler access their service which addition returning relevant hits also returns the appropriate advertisement Another solution involves the MetaCrawler accepting advertisements and forming profit sharing relationship with the service providers are currently investigating these and other methods mutually beneficial existence with service providers Conclusions this paper have presented the MetaCrawler meta service for Web searching with additional features designed return more references higher quality than standard search services demonstrated that users follow references reported variety different search services confirming that single service not sufficient Table Further due the expressive power the MetaCrawler interface the MetaCrawler was able automatically determine that the hits returned can discarded Finally the performance benchmarks and usage logs also show that the features provided the MetaCrawler are both reasonably fast and actually used practice The MetaCrawler provides Consumer Reports sorts for Web searchers The individual service data extracted from the MetaCrawler logs compelling evidence concerning the quality each service comparing services using the same query text and recording what links users follow are able evaluate the services from user point view far know are the first quantitatively compare the search services used MetaCrawler large sample authentic user queries While possible that some MetaCrawler features could integrated into the search services others are intrinsic meta services definition only meta service can provide the coverage gained using multiple services Also argued earlier client side meta services can offer user and site customizations and absorb the load caused post processing search results Finally there are some features that not belong under control search services for purely pragmatic reasons For example more commercial search services become available tools will emerge that select which services use the basis cost impartial meta service such the MetaCrawler avoids the conflict interest that would arise such tool were offered one the commercial services New Web services are constantly being created the number and variety services grows natural group existing services under one umbrella The MetaCrawler goes further than merely organizing services creating integrated meta service that moves the interface and the associated computational load closer the user believe that this trend moving the information food chain will continue The MetaCrawler one the first popular meta services but many more will follow Acknowledgments The research presented this paper could not have been accomplished without the help many individuals would like thank Mary Kaye Rodgers for editing assistance and for putting with late nights Ruth Etzioni and Ellen Spertus provided comments earlier draft Dan Weld Rich Segal Keith Golden George Forman and Donald Chinn were very vocal and active testing the early prototypes the MetaCrawler and Craig Horman and Nancy Johnson Burr were extremely helpful and patient dealing with when ran amok Lara Lewis was very helpful finding references upon demand The Internet Softbot group provided early insight into desirable features the MetaCrawler and Brian Bershad and Hank Levy contributed ideas relating the impact the MetaCrawler could have the Web Ken Waln aided early development for his form patches the WWW library and Lou Montulli helped later development unlocking the secrets nph scripts and Netscape caching MetaCrawler development was supported gifts from West and Rockwell International Palo Alto Research Etzioni Softbot research supported Office Naval Research grant and National Science Foundation grant IRI References InfoSeek Corporation InfoSeek Home Page URL http www infoseek com William Cross All One Internet Search Page URL http www albany net wcross all srch html Daniel Dreilinger Savvy Search Home Page URL http www colostate edu dreiling smartform html Daniel Dreilinger Integrating Heterogeneous WWW Search Engines URL ftp savvy report May EINet Galaxy Home Page URL http galaxy einet net galaxy html Mic Bowman Harvest Scalable Customizable Discovery and Access System Technical Report Department Computer Science University Colorado Boulder Colorado March URL http harvest colorado edu harvest papers html Michael Schwartz WWW Home Pages Harvest Broker URL http town hall org Harvest brokers www home pages Etzioni and Weld softbot based interface the internet CACM July URL http www washington edu research softbots David Filo and Jerry Yang Yahoo Home Page URL http www yahoo com James Gosling and Henry McGilton The Java Language Environment White Paper URL http java sun com whitePaper javawhitepaper html IBM Inc infoMarket Search Home Page URL http www infomkt ibm com IBM Inc Query Image Content Home Page URL http wwwqbic almaden ibm com qbic qbic html Martijn Koster Robots the Web threat treat ConneXions April LEXIS NEXIS LEXIS NEXIS Communication Center URL http www lexis nexis com Michael Mauldin Lycos Home Page URL http lycos cmu edu Michael Mauldin and John Leavitt Web Agent Related Research the Center for Machine Translation Proceedings SIGNIDR McLean Virginia August Max Metral Helpful Online Music Recommendation Service URL http media mit edu ringo ringo html Nexor CUSI Configurable Universal Search Interface URL http pubweb nexor public cusi cusi html University Geneva Search Engines URL http cuiwww unige meta index html Open Text Inc Open Text Web Index Home Page URL http www opentext com omw omw html Personal Library Software Inc Personal Library Software Home Page URL http www pls com Brian Pinkerton WebCrawler Home Page URL http webcrawler com Brian Pinkerton Finding What People Want Experiences with the WebCrawler Proceedings the Second World Wide Web Conference Mosaic and the Web Chicago USA October Brandon Plewe The Virtual Tourist Home Page URL http wings buffalo edu world Daniel Sears Guide CodeWarrior Magic MPW Development Release URL http www genmagic com MagicCapDocs CodeWarriorMagic introduction html May DejaNews Research Service DejaNews Home Page URL http www dejanews com Sun Microsystems Inc Multithreaded Query Page URL http www sun com cgi bin show search mtquery index body The Internet Movie Database Team The Internet Movie Database URL http www msstate edu The McKinley Group Inc Magellan McKinley Internet Directory URL http www mckinley com Verity Inc Verity Home Page URL http www verity com About the Authors Erik Selberg selberg washington edu http www washington edu homes selberg Department Computer Science and Engineering Box University Washington Seattle Erik Selberg pursuing his computer science the University Washington His primary research area involves World Wide Web search although also has interests regarding system performance and security well multi agent coordination and planning April created the MetaCrawler parallel Web search meta service graduated from Carnegie Mellon University with double major computer science and logic and received the first Allen Newell Award for Excellence Undergraduate Research Oren Etzioni etzioni washington edu http www washington edu homes etzioni Department Computer Science and Engineering Box University Washington Seattle Oren Etzioni received his bachelor degree computer science from Harvard University June and his from Carnegie Mellon University January joined the University Washington assistant professor computer science and engineering February the fall launched the Internet Softbots project Etzioni received NSF Young Investigator Award Etzioni was chosen one finalists the Discover Awards for Technological Innovation Computer Software for his work Internet Softbots His research interests include software agents machine learning and human computer interaction About this document Multi Service Search and Comparison Using the MetaCrawler This document was generated using the LaTeX HTML translator Version Fri Jan Copyright Nikos Drakos Computer Based Learning Unit University Leeds The command line arguments were latex html split www final tex The translation was initiated Erik Selberg Mon Oct PDT Erik Selberg Mon Oct PDT 